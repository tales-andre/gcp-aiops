[
  {
    "name": "OOMKilled",
    "slug": "oomkilled",
    "aliases": [
      "Killed process",
      "OOMKilled",
      "Out of memory",
      "signal: killed"
    ],
    "category": "Pod Runtime",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'OOMKilled' geralmente indica um problema na categoria Pod Runtime",
    "signals": {
      "log_keywords": [
        "OOMKilled",
        "Out of memory",
        "Killed process",
        "memory cgroup out of memory",
        "signal: killed",
        "evicted: The node was low on resource: memory"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Limite de memória (resources.limits.memory) menor que o uso real do processo.",
        "Spikes de alocação (cache, buffers, JIT ou GC) não refletidos em requests.",
        "Vazamentos ou memory bloat (por exemplo: grandes batches, buffers não liberados)."
      ],
      "immediate_actions": [
        "Verifique o motivo exato do OOM nos eventos: kubectl describe pod <pod> -n <ns> (campo Last State / Reason).",
        "Confira reinícios: kubectl get pod <pod> -n <ns> -o wide e kubectl logs <pod> -n <ns> --previous.",
        "Observe consumo: instale metrics-server e rode kubectl top pod <pod> -n <ns>.",
        "Se JVM: adicione flags que respeitem cgroups (ex.: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75);",
        "se Node.js: ajuste --max-old-space-size; se Python: reduza workers/batch size."
      ],
      "best_practices": [
        "Defina requests/limits realistas com base em métricas históricas (P95/P99).",
        "Ative HPA/VPA quando aplicável; use limites para evitar ruído entre pods.",
        "Instrumente a aplicação (heap profiles) e monitore GC/leaks.",
        "Use liveness/readiness/startup probes para estabilidade durante picos."
      ],
      "storage_diagnostics": [
        "Não se aplica diretamente; verifique apenas se volumes temporários não induzem swap em memória."
      ],
      "corrections": [
        "Dimensione corretamente: aumente requests para o pico de uso e limits com margem (10–30%).",
        "Otimize a aplicação: reduza paralelismo, batch size, buffers; corrija vazamentos.",
        "Autoscaling: considere Vertical Pod Autoscaler (VPA) e HPA baseado em memória onde aplicável.",
        "Probes: evite que liveness mate o pod durante warmup — use startupProbe e tempos mais generosos."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl top pod <pod> -n <ns>",
      "kubectl top node"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl top node",
        "kubectl top pod <pod> -n <ns>"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "OOMKilled por pod (últimas 6h)",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[6h]))"
        },
        {
          "title": "Uso de memória por container",
          "expr": "avg_over_time(container_memory_working_set_bytes[5m])"
        }
      ]
    },
    "remediations": [
      "Ajuste `resources.requests/limits.memory` baseado no pico observado.",
      "Reduza *batch size*, paralelismo e *caches*; aplique flags de memória do runtime (JVM/Node/Python).",
      "Implemente `startupProbe` para proteger a inicialização e evite *liveness* prematura.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Ajuste requests/limits de CPU/Memória, comandos/args e dependências iniciais.",
      "Cheque volumes/paths/permissions exigidos pelo container."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Há picos de CPU/Memória ou erros específicos nos logs?",
      "As probes estão ajustadas (startup/liveness/readiness)?"
    ],
    "faq": [
      {
        "q": "O que significa 'OOMKilled' no Kubernetes?",
        "a": "'OOMKilled' geralmente indica um problema na categoria Pod Runtime"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CrashLoopBackOff",
    "slug": "crashloopbackoff",
    "aliases": [
      "CrashLoopBackOff"
    ],
    "category": "Pod Runtime",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CrashLoopBackOff' geralmente indica um problema na categoria Pod Runtime",
    "signals": {
      "log_keywords": [
        "CrashLoopBackOff",
        "Back-off restarting failed container",
        "back-off restarting failed container"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falha no entrypoint/command, dependência externa indisponível, config ausente, ou panic da aplicação.",
        "Probes agressivas derrubando o processo durante a inicialização."
      ],
      "immediate_actions": [
        "Veja o contêiner anterior: kubectl logs <pod> -n <ns> --previous (indica a causa de saída).",
        "Descreva o pod e eventos: kubectl describe pod <pod> -n <ns> (erros de mount/env/args)."
      ],
      "best_practices": [
        "Valide variáveis/Secrets/ConfigMaps antes do deploy.",
        "Use startupProbe para apps pesadas; dê tempo para aquecimento.",
        "Padronize healthchecks e exponha métricas para observabilidade.",
        "Mantenha logs estruturados para facilitar triagem."
      ],
      "storage_diagnostics": [
        "Se envolver volumes, verifique permissões, paths e montagem antes do processo iniciar."
      ],
      "corrections": [
        "Corrija env/args/arquivos requeridos; torne scripts executáveis (chmod +x).",
        "Se devido a readiness/liveness, relaxe initialDelaySeconds, periodSeconds, timeoutSeconds e failureThreshold.",
        "Aumente terminationGracePeriodSeconds se precisar finalizar com graceful shutdown."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "CrashLoopBackOff por pod",
          "expr": "sum by (namespace,pod) (kube_pod_container_status_waiting_reason{reason=\"CrashLoopBackOff\"})"
        }
      ]
    },
    "remediations": [
      "Corrija `command`/`args`/`entrypoint`; torne scripts executáveis.",
      "Revise *probes* e aumente tempos/thresholds conforme necessidade.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Ajuste requests/limits de CPU/Memória, comandos/args e dependências iniciais.",
      "Cheque volumes/paths/permissions exigidos pelo container."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Há picos de CPU/Memória ou erros específicos nos logs?",
      "As probes estão ajustadas (startup/liveness/readiness)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CrashLoopBackOff' no Kubernetes?",
        "a": "'CrashLoopBackOff' geralmente indica um problema na categoria Pod Runtime"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CreateContainerConfigError",
    "slug": "createcontainerconfigerror",
    "aliases": [
      "CreateContainerConfigError",
      "configmap not found"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CreateContainerConfigError' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "CreateContainerConfigError",
        "Error creating: config error",
        "configmap not found",
        "secret .* not found",
        "re:Error from server \\(NotFound\\): secrets? .* not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CreateContainerConfigError' no Kubernetes?",
        "a": "'CreateContainerConfigError' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CreateContainerError",
    "slug": "createcontainererror",
    "aliases": [
      "CreateContainerError"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CreateContainerError' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "CreateContainerError",
        "Error: failed to create containerd task",
        "re:failed to create container .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CreateContainerError' no Kubernetes?",
        "a": "'CreateContainerError' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "RunContainerError",
    "slug": "runcontainererror",
    "aliases": [
      "RunContainerError"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'RunContainerError' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "RunContainerError",
        "re:failed to start container .*",
        "re:OCI runtime create failed.*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'RunContainerError' no Kubernetes?",
        "a": "'RunContainerError' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Error: failed to start container",
    "slug": "error-failed-to-start-container",
    "aliases": [
      "Error",
      "Error: failed to start container",
      "container",
      "failed",
      "start"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Error: failed to start container' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Error: failed to start container",
        "re:executable file not found in \\$PATH",
        "permission denied while trying to connect to the Docker daemon socket"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Error: failed to start container' no Kubernetes?",
        "a": "'Error: failed to start container' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ImagePullBackOff",
    "slug": "imagepullbackoff",
    "aliases": [
      "ErrImagePull",
      "ImagePullBackOff",
      "back-off pulling image"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ImagePullBackOff' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "ImagePullBackOff",
        "back-off pulling image",
        "ErrImagePull"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Use tags imutáveis (SHA digest) e política imagePullPolicy adequada.",
        "Automatize login/secret via controlador (Workload Identity/IRSA).",
        "Evite 'latest'; versionamento semântico ajuda rollback.",
        "Monitore taxa de erro de pull."
      ],
      "storage_diagnostics": [
        "Não aplicável diretamente; apenas confirme espaço em disco do nó para camadas da imagem."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Falha ao puxar imagem",
          "expr": "sum by (namespace,pod) (kube_pod_container_status_waiting_reason{reason=~\"ImagePullBackOff|ErrImagePull\"})"
        }
      ]
    },
    "remediations": [
      "Corrija `command`/`args`/`entrypoint`; torne scripts executáveis.",
      "Revise *probes* e aumente tempos/thresholds conforme necessidade.",
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'ImagePullBackOff' no Kubernetes?",
        "a": "'ImagePullBackOff' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "InvalidImageName",
    "slug": "invalidimagename",
    "aliases": [
      "InvalidImageName",
      "invalid reference format"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'InvalidImageName' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "InvalidImageName",
        "invalid reference format",
        "Error response from daemon: pull access denied for"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Falha ao puxar imagem",
          "expr": "sum by (namespace,pod) (kube_pod_container_status_waiting_reason{reason=~\"ImagePullBackOff|ErrImagePull\"})"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'InvalidImageName' no Kubernetes?",
        "a": "'InvalidImageName' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ErrImageNeverPull",
    "slug": "errimageneverpull",
    "aliases": [
      "ErrImageNeverPull"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ErrImageNeverPull' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "ErrImageNeverPull",
        "re:image pull policy is Never.*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Falha ao puxar imagem",
          "expr": "sum by (namespace,pod) (kube_pod_container_status_waiting_reason{reason=~\"ImagePullBackOff|ErrImagePull\"})"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ErrImageNeverPull' no Kubernetes?",
        "a": "'ErrImageNeverPull' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Readiness probe failed",
    "slug": "readiness-probe-failed",
    "aliases": [
      "Readiness",
      "Readiness probe failed",
      "failed",
      "probe"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Readiness probe failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Readiness probe failed",
        "re:readiness probe failed: .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Path/porta incorretos nas probes.",
        "Timeout/intervalo agressivo em apps lentas.",
        "TLS/autenticação exigidos pelo endpoint de saúde.",
        "Dependência externa lenta causando falha na saúde."
      ],
      "immediate_actions": [
        "Descreva o pod e veja falhas de probe.",
        "Teste manualmente o endpoint de saúde via kubectl exec (curl).",
        "Aumente temporariamente timeoutSeconds/periodSeconds/failureThreshold.",
        "Habilite startupProbe quando houver cold start."
      ],
      "best_practices": [
        "startupProbe protege a fase de warmup; somente depois habilite liveness.",
        "Use httpGet/tcpSocket válidos e portas corretas (containerPort ↔ targetPort).",
        "Ajuste timeoutSeconds e thresholds para evitar falsos positivos sob carga."
      ],
      "storage_diagnostics": [
        "Não aplicável; probes não usam storage."
      ],
      "corrections": [
        "Corrija paths/portas e autenticação.",
        "Ajuste janelas de timeout/period/failureThreshold.",
        "Implemente startupProbe para inicializações lentas.",
        "Desacople dependências ou trate timeouts/retries."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `httpGet.path`/`port` ou `tcpSocket`; alinhe `containerPort` e `targetPort`.",
      "Use `startupProbe` antes da *liveness* para apps lentas.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Readiness probe failed' no Kubernetes?",
        "a": "'Readiness probe failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Liveness probe failed",
    "slug": "liveness-probe-failed",
    "aliases": [
      "Liveness",
      "Liveness probe failed",
      "failed",
      "probe"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Liveness probe failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Liveness probe failed",
        "re:liveness probe failed: .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Path/porta incorretos nas probes.",
        "Timeout/intervalo agressivo em apps lentas.",
        "TLS/autenticação exigidos pelo endpoint de saúde.",
        "Dependência externa lenta causando falha na saúde."
      ],
      "immediate_actions": [
        "Descreva o pod e veja falhas de probe.",
        "Teste manualmente o endpoint de saúde via kubectl exec (curl).",
        "Aumente temporariamente timeoutSeconds/periodSeconds/failureThreshold.",
        "Habilite startupProbe quando houver cold start."
      ],
      "best_practices": [
        "startupProbe protege a fase de warmup; somente depois habilite liveness.",
        "Use httpGet/tcpSocket válidos e portas corretas (containerPort ↔ targetPort).",
        "Ajuste timeoutSeconds e thresholds para evitar falsos positivos sob carga."
      ],
      "storage_diagnostics": [
        "Não aplicável; probes não usam storage."
      ],
      "corrections": [
        "Corrija paths/portas e autenticação.",
        "Ajuste janelas de timeout/period/failureThreshold.",
        "Implemente startupProbe para inicializações lentas.",
        "Desacople dependências ou trate timeouts/retries."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `httpGet.path`/`port` ou `tcpSocket`; alinhe `containerPort` e `targetPort`.",
      "Use `startupProbe` antes da *liveness* para apps lentas.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Liveness probe failed' no Kubernetes?",
        "a": "'Liveness probe failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Startup probe failed",
    "slug": "startup-probe-failed",
    "aliases": [
      "Startup",
      "Startup probe failed",
      "failed",
      "probe"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Startup probe failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Startup probe failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Path/porta incorretos nas probes.",
        "Timeout/intervalo agressivo em apps lentas.",
        "TLS/autenticação exigidos pelo endpoint de saúde.",
        "Dependência externa lenta causando falha na saúde."
      ],
      "immediate_actions": [
        "Descreva o pod e veja falhas de probe.",
        "Teste manualmente o endpoint de saúde via kubectl exec (curl).",
        "Aumente temporariamente timeoutSeconds/periodSeconds/failureThreshold.",
        "Habilite startupProbe quando houver cold start."
      ],
      "best_practices": [
        "startupProbe protege a fase de warmup; somente depois habilite liveness.",
        "Use httpGet/tcpSocket válidos e portas corretas (containerPort ↔ targetPort).",
        "Ajuste timeoutSeconds e thresholds para evitar falsos positivos sob carga."
      ],
      "storage_diagnostics": [
        "Não aplicável; probes não usam storage."
      ],
      "corrections": [
        "Corrija paths/portas e autenticação.",
        "Ajuste janelas de timeout/period/failureThreshold.",
        "Implemente startupProbe para inicializações lentas.",
        "Desacople dependências ou trate timeouts/retries."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `httpGet.path`/`port` ou `tcpSocket`; alinhe `containerPort` e `targetPort`.",
      "Use `startupProbe` antes da *liveness* para apps lentas.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Startup probe failed' no Kubernetes?",
        "a": "'Startup probe failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "FailedScheduling",
    "slug": "failedscheduling",
    "aliases": [
      "FailedScheduling"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'FailedScheduling' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "FailedScheduling",
        "re:0/\\d+ nodes are available",
        "No nodes are available that match all of the predicates"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Pods não agendados (unschedulable)",
          "expr": "sum by (namespace,pod) (kube_pod_status_unschedulable)"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'FailedScheduling' no Kubernetes?",
        "a": "'FailedScheduling' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Insufficient cpu",
    "slug": "insufficient-cpu",
    "aliases": [
      "Insufficient",
      "Insufficient cpu",
      "cpu"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Insufficient cpu' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Insufficient cpu",
        "re:0/\\d+ nodes are available: .* Insufficient cpu"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Pods não agendados (unschedulable)",
          "expr": "sum by (namespace,pod) (kube_pod_status_unschedulable)"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Insufficient cpu' no Kubernetes?",
        "a": "'Insufficient cpu' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Insufficient memory",
    "slug": "insufficient-memory",
    "aliases": [
      "Insufficient",
      "Insufficient memory",
      "memory"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Insufficient memory' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Insufficient memory",
        "re:0/\\d+ nodes are available: .* Insufficient memory"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "OOMKilled por pod (últimas 6h)",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[6h]))"
        },
        {
          "title": "Uso de memória por container",
          "expr": "avg_over_time(container_memory_working_set_bytes[5m])"
        },
        {
          "title": "Pods não agendados (unschedulable)",
          "expr": "sum by (namespace,pod) (kube_pod_status_unschedulable)"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Insufficient memory' no Kubernetes?",
        "a": "'Insufficient memory' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node selector mismatch",
    "slug": "node-selector-mismatch",
    "aliases": [
      "Node",
      "Node selector mismatch",
      "mismatch",
      "selector"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node selector mismatch' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "didn't match node selector",
        "re:node\\(s\\) didn't match node selector"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node selector mismatch' no Kubernetes?",
        "a": "'Node selector mismatch' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Taint requires toleration",
    "slug": "taint-requires-toleration",
    "aliases": [
      "NoSchedule",
      "Taint",
      "Taint requires toleration",
      "requires",
      "toleration"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Taint requires toleration' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NoSchedule",
        "re:node\\(s\\) had taint .* that the pod didn't tolerate"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Taint requires toleration' no Kubernetes?",
        "a": "'Taint requires toleration' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Volume node affinity conflict",
    "slug": "volume-node-affinity-conflict",
    "aliases": [
      "Volume",
      "Volume node affinity conflict",
      "affinity",
      "conflict",
      "node"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Volume node affinity conflict' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "volume node affinity conflict",
        "re:node\\(s\\) had volume node affinity conflict"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Volume node affinity conflict' no Kubernetes?",
        "a": "'Volume node affinity conflict' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Unbound PVC blocks scheduling",
    "slug": "unbound-pvc-blocks-scheduling",
    "aliases": [
      "PVC",
      "Unbound",
      "Unbound PVC blocks scheduling",
      "blocks",
      "scheduling"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Unbound PVC blocks scheduling' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "pod has unbound immediate PersistentVolumeClaims"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pvc <pvc> -n <ns> e confira eventos.",
        "kubectl get sc,pv -A e verifique disponibilidade/labels.",
        "Ajuste requests de storage e accessModes.",
        "Confirme zone/allowedTopologies/NodeAffinity do PV."
      ],
      "best_practices": [
        "Use StorageClass padrão e tamanhos padronizados.",
        "Evite selectors desnecessários; prefira dinâmico.",
        "Monitore binding time e erros do provisioner.",
        "Defina quotas/limites por namespace."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento.",
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Unbound PVC blocks scheduling' no Kubernetes?",
        "a": "'Unbound PVC blocks scheduling' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PVC not bound",
    "slug": "pvc-not-bound",
    "aliases": [
      "PVC",
      "PVC not bound",
      "bound",
      "not"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'PVC not bound' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "persistentvolumeclaim .* is not bound",
        "re:persistentvolumeclaim \\\".*\\\" is not bound"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "StorageClass ausente/errada ou provisioner indisponível.",
        "Requisitos de acesso/tamanho/selector incompatíveis com PVs.",
        "Zonas/afinidade de volume não casam com o nó.",
        "Cotas de storage esgotadas."
      ],
      "immediate_actions": [
        "kubectl describe pvc <pvc> -n <ns> e confira eventos.",
        "kubectl get sc,pv -A e verifique disponibilidade/labels.",
        "Ajuste requests de storage e accessModes.",
        "Confirme zone/allowedTopologies/NodeAffinity do PV."
      ],
      "best_practices": [
        "Use StorageClass padrão e tamanhos padronizados.",
        "Evite selectors desnecessários; prefira dinâmico.",
        "Monitore binding time e erros do provisioner.",
        "Defina quotas/limites por namespace."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PVC not bound' no Kubernetes?",
        "a": "'PVC not bound' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "WaitForFirstConsumer",
    "slug": "waitforfirstconsumer",
    "aliases": [
      "WaitForFirstConsumer"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'WaitForFirstConsumer' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "waiting for first consumer to be created before binding"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'WaitForFirstConsumer' no Kubernetes?",
        "a": "'WaitForFirstConsumer' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "No PVs and no StorageClass",
    "slug": "no-pvs-and-no-storageclass",
    "aliases": [
      "No PVs and no StorageClass",
      "PVs",
      "StorageClass",
      "and"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'No PVs and no StorageClass' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "no persistent volumes available for this claim and no storage class is set"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'No PVs and no StorageClass' no Kubernetes?",
        "a": "'No PVs and no StorageClass' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "MountVolume.SetUp failed",
    "slug": "mountvolume-setup-failed",
    "aliases": [
      "MountVolume",
      "MountVolume.SetUp failed",
      "SetUp",
      "failed"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'MountVolume.SetUp failed' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "MountVolume.SetUp failed for volume",
        "re:MountVolume.SetUp failed for volume \\\".*\\\": .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Credenciais/secret do CSI inválidos.",
        "Permissões de filesystem (UID/GID) incompatíveis.",
        "AccessMode incompatível (ex.: ReadWriteOnce em múltiplos pods).",
        "Path de montagem incorreto ou readOnly inesperado."
      ],
      "immediate_actions": [
        "Descreva o pod e o PVC/PV para eventos do CSI.",
        "Confirme fsGroup/runAsUser e permissões do path.",
        "Valide secrets/params do volume (ex.: NFS, S3, Filestore).",
        "Teste montagem com pod utilitário (busybox/alpine)."
      ],
      "best_practices": [
        "Padronize fsGroup e permissões.",
        "Documente AccessModes por tipo de workload.",
        "Evite execução como root quando possível; use SecurityContext.",
        "Monitore erros do driver CSI."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'MountVolume.SetUp failed' no Kubernetes?",
        "a": "'MountVolume.SetUp failed' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "MountVolume.MountDevice failed",
    "slug": "mountvolume-mountdevice-failed",
    "aliases": [
      "MountDevice",
      "MountVolume",
      "MountVolume.MountDevice failed",
      "failed"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'MountVolume.MountDevice failed' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "MountVolume.MountDevice failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Credenciais/secret do CSI inválidos.",
        "Permissões de filesystem (UID/GID) incompatíveis.",
        "AccessMode incompatível (ex.: ReadWriteOnce em múltiplos pods).",
        "Path de montagem incorreto ou readOnly inesperado."
      ],
      "immediate_actions": [
        "Descreva o pod e o PVC/PV para eventos do CSI.",
        "Confirme fsGroup/runAsUser e permissões do path.",
        "Valide secrets/params do volume (ex.: NFS, S3, Filestore).",
        "Teste montagem com pod utilitário (busybox/alpine)."
      ],
      "best_practices": [
        "Padronize fsGroup e permissões.",
        "Documente AccessModes por tipo de workload.",
        "Evite execução como root quando possível; use SecurityContext.",
        "Monitore erros do driver CSI."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'MountVolume.MountDevice failed' no Kubernetes?",
        "a": "'MountVolume.MountDevice failed' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Multi-Attach error",
    "slug": "multi-attach-error",
    "aliases": [
      "Multi-Attach",
      "Multi-Attach error",
      "error"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Multi-Attach error' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Multi-Attach error for volume"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Multi-Attach error' no Kubernetes?",
        "a": "'Multi-Attach error' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Read-only file system",
    "slug": "read-only-file-system",
    "aliases": [
      "EROFS",
      "Read-only",
      "Read-only file system",
      "file",
      "system"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Read-only file system' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Read-only file system",
        "EROFS"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Credenciais/secret do CSI inválidos.",
        "Permissões de filesystem (UID/GID) incompatíveis.",
        "AccessMode incompatível (ex.: ReadWriteOnce em múltiplos pods).",
        "Path de montagem incorreto ou readOnly inesperado."
      ],
      "immediate_actions": [
        "Descreva o pod e o PVC/PV para eventos do CSI.",
        "Confirme fsGroup/runAsUser e permissões do path.",
        "Valide secrets/params do volume (ex.: NFS, S3, Filestore).",
        "Teste montagem com pod utilitário (busybox/alpine)."
      ],
      "best_practices": [
        "Padronize fsGroup e permissões.",
        "Documente AccessModes por tipo de workload.",
        "Evite execução como root quando possível; use SecurityContext.",
        "Monitore erros do driver CSI."
      ],
      "storage_diagnostics": [
        "Logs do driver CSI e eventos do kubelet relativos ao volume."
      ],
      "corrections": [
        "Corrigir fsGroup/UID/GID e permissões do diretório.",
        "Usar AccessMode compatível com o padrão de uso.",
        "Ajustar secrets/parâmetros do volume.",
        "Conferir flags readOnly e mountPath no spec."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Read-only file system' no Kubernetes?",
        "a": "'Read-only file system' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "permission denied",
    "slug": "permission-denied",
    "aliases": [
      "EACCES",
      "EPERM",
      "denied",
      "permission",
      "permission denied"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'permission denied' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "permission denied",
        "EPERM",
        "EACCES"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'permission denied' no Kubernetes?",
        "a": "'permission denied' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "no such file or directory",
    "slug": "no-such-file-or-directory",
    "aliases": [
      "ENOENT",
      "directory",
      "file",
      "no such file or directory",
      "such"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'no such file or directory' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "no such file or directory",
        "ENOENT"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'no such file or directory' no Kubernetes?",
        "a": "'no such file or directory' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "operation not permitted",
    "slug": "operation-not-permitted",
    "aliases": [
      "not",
      "operation",
      "operation not permitted",
      "permitted"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'operation not permitted' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "operation not permitted"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'operation not permitted' no Kubernetes?",
        "a": "'operation not permitted' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "No endpoints for service",
    "slug": "no-endpoints-for-service",
    "aliases": [
      "No endpoints for service",
      "endpoints",
      "for",
      "service"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'No endpoints for service' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "re:no endpoints available for service\\s+\"?[\\w\\-\\.]+\"?"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'No endpoints for service' no Kubernetes?",
        "a": "'No endpoints for service' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "connection refused",
    "slug": "connection-refused",
    "aliases": [
      "ECONNREFUSED",
      "connection",
      "connection refused",
      "refused"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'connection refused' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "connection refused",
        "ECONNREFUSED"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Não aplicável; foco é rede."
      ],
      "corrections": [
        "Criar/ajustar Service e selectors; garantir endpoints prontos.",
        "Abrir NetworkPolicies para o fluxo necessário.",
        "Conferir portas/alvos corretos (targetPort/port).",
        "Configurar rotas/firewall para egress."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'connection refused' no Kubernetes?",
        "a": "'connection refused' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "i/o timeout",
    "slug": "i-o-timeout",
    "aliases": [
      "ETIMEDOUT",
      "context deadline exceeded",
      "i/o",
      "i/o timeout",
      "timeout"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'i/o timeout' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "i/o timeout",
        "ETIMEDOUT",
        "context deadline exceeded"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Não aplicável; foco é rede."
      ],
      "corrections": [
        "Criar/ajustar Service e selectors; garantir endpoints prontos.",
        "Abrir NetworkPolicies para o fluxo necessário.",
        "Conferir portas/alvos corretos (targetPort/port).",
        "Configurar rotas/firewall para egress."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'i/o timeout' no Kubernetes?",
        "a": "'i/o timeout' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "DNS no such host",
    "slug": "dns-no-such-host",
    "aliases": [
      "DNS",
      "DNS no such host",
      "host",
      "no such host",
      "such"
    ],
    "category": "Networking/DNS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "re:lookup .* on .*: no such host",
        "no such host"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FQDN/hostname incorreto ou sem sufixo de busca (.svc.cluster.local).",
        "Service inexistente ou sem endpoints (selectors não batem).",
        "CoreDNS com ConfigMap inválido (forward/stubDomains) ou pods em CrashLoop/sem recursos.",
        "NetworkPolicy bloqueando egress UDP/TCP 53 para kube-dns.",
        "Resolver upstream fora do ar/sem rota (firewall/VPC/peering/proxy).",
        "dnsPolicy/dnsConfig do Pod incorretas ou hostNetwork sem ClusterFirstWithHostNet.",
        "/etc/hosts dentro do container sobrepõe resolução."
      ],
      "immediate_actions": [
        "Verifique CoreDNS e logs: kubectl -n kube-system get pods -l k8s-app=kube-dns && kubectl -n kube-system logs deploy/coredns --tail=200",
        "Cheque Service/Endpoints: kubectl get svc,ep,endpointslices <service> -n <ns>",
        "Teste resolução no Pod: kubectl exec -it <pod> -n <ns> -- sh -c \"getent hosts <host> || nslookup <host> || dig +short <host>\"",
        "Valide NetworkPolicies liberando UDP/TCP 53 ao kube-dns.",
        "Teste FQDN completo do Service: <svc>.<ns>.svc.cluster.local."
      ],
      "best_practices": [
        "Use FQDN para Services internos e domínios absolutos para externos.",
        "Monitore latência/erros do CoreDNS; ajuste requests/limits e HPA.",
        "Mantenha ConfigMap do CoreDNS simples e validado.",
        "Evite entradas fixas em /etc/hosts nas imagens.",
        "Em hostNetwork, use dnsPolicy: ClusterFirstWithHostNet."
      ],
      "storage_diagnostics": [
        "Não se aplica a DNS; priorize logs do CoreDNS e verificação de NetworkPolicy."
      ],
      "corrections": [
        "Corrija typos e use FQDN .svc.cluster.local quando for Service.",
        "Ajuste selectors do Service para gerar Endpoints válidos.",
        "Permita egress UDP/TCP 53 (kube-dns) nas NetworkPolicies.",
        "Corrija forward/stubDomains no CoreDNS e faça rollout restart.",
        "Ajuste dnsPolicy/dnsConfig (ou ClusterFirstWithHostNet) conforme necessário.",
        "Remova entradas conflitantes em /etc/hosts na imagem/container."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'DNS no such host' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ClusterIP unreachable",
    "slug": "clusterip-unreachable",
    "aliases": [
      "ClusterIP",
      "ClusterIP service unreachable",
      "ClusterIP unreachable",
      "re:Cilium.*service.*unreachable",
      "unreachable"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ClusterIP unreachable' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "ClusterIP service unreachable",
        "KUBE-SVC iptables rules missing",
        "re:Cilium.*service.*unreachable"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ClusterIP unreachable' no Kubernetes?",
        "a": "'ClusterIP unreachable' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CNI plugin not ready",
    "slug": "cni-plugin-not-ready",
    "aliases": [
      "CNI",
      "CNI plugin not ready",
      "cni config uninitialized",
      "not",
      "plugin",
      "ready"
    ],
    "category": "CNI/Networking",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'CNI plugin not ready' geralmente indica um problema na categoria CNI/Networking",
    "signals": {
      "log_keywords": [
        "NetworkPlugin cni failed to set up pod",
        "cni config uninitialized",
        "CNI plugin not initialized"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CNI plugin not ready' no Kubernetes?",
        "a": "'CNI plugin not ready' geralmente indica um problema na categoria CNI/Networking"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Failed to create pod sandbox",
    "slug": "failed-to-create-pod-sandbox",
    "aliases": [
      "Failed",
      "Failed to create pod sandbox",
      "create",
      "pod",
      "sandbox"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Failed to create pod sandbox' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "failed to create pod sandbox: rpc error",
        "re:failed to setup network for sandbox.*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Failed to create pod sandbox' no Kubernetes?",
        "a": "'Failed to create pod sandbox' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "port is already allocated",
    "slug": "port-is-already-allocated",
    "aliases": [
      "allocated",
      "already",
      "port",
      "port is already allocated"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'port is already allocated' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "port is already allocated",
        "Bind for 0.0.0.0:* failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'port is already allocated' no Kubernetes?",
        "a": "'port is already allocated' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Ingress default backend 404",
    "slug": "ingress-default-backend-404",
    "aliases": [
      "404",
      "Ingress",
      "Ingress default backend 404",
      "backend",
      "default"
    ],
    "category": "Ingress",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Ingress default backend 404' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "default backend - 404",
        "404 Not Found from Ingress"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Ingress default backend 404' no Kubernetes?",
        "a": "'Ingress default backend 404' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Ingress TLS secret not found",
    "slug": "ingress-tls-secret-not-found",
    "aliases": [
      "Ingress",
      "Ingress TLS secret not found",
      "TLS",
      "found",
      "not",
      "secret"
    ],
    "category": "Ingress",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Ingress TLS secret not found' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "re:secret \\\".*\\\" not found",
        "tls: private key does not match public certificate"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Ingress TLS secret not found' no Kubernetes?",
        "a": "'Ingress TLS secret not found' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Ingress upstream connect error",
    "slug": "ingress-upstream-connect-error",
    "aliases": [
      "Ingress",
      "Ingress upstream connect error",
      "connect",
      "error",
      "upstream"
    ],
    "category": "Ingress",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Ingress upstream connect error' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "upstream connect error or disconnect/reset before headers",
        "re:upstream .* connection failure"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Ingress upstream connect error' no Kubernetes?",
        "a": "'Ingress upstream connect error' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "x509 unknown authority",
    "slug": "x509-unknown-authority",
    "aliases": [
      "authority",
      "unknown",
      "x509",
      "x509 unknown authority"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'x509 unknown authority' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "x509: certificate signed by unknown authority"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'x509 unknown authority' no Kubernetes?",
        "a": "'x509 unknown authority' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "x509 expired or not yet valid",
    "slug": "x509-expired-or-not-yet-valid",
    "aliases": [
      "expired",
      "not",
      "valid",
      "x509",
      "x509 expired or not yet valid",
      "yet"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'x509 expired or not yet valid' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "x509: certificate has expired or is not yet valid"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'x509 expired or not yet valid' no Kubernetes?",
        "a": "'x509 expired or not yet valid' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "TLS handshake error",
    "slug": "tls-handshake-error",
    "aliases": [
      "TLS",
      "TLS handshake error",
      "error",
      "handshake"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'TLS handshake error' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "TLS handshake error",
        "remote error: tls: bad certificate"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'TLS handshake error' no Kubernetes?",
        "a": "'TLS handshake error' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Forbidden list resource",
    "slug": "forbidden-list-resource",
    "aliases": [
      "Forbidden",
      "Forbidden list resource",
      "list",
      "resource"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Forbidden list resource' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "Forbidden: cannot list resource",
        "re:forbidden: User .* cannot list resource"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Forbidden list resource' no Kubernetes?",
        "a": "'Forbidden list resource' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "SA cannot get resource",
    "slug": "sa-cannot-get-resource",
    "aliases": [
      "SA cannot get resource",
      "cannot",
      "get",
      "resource"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'SA cannot get resource' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "re:User \\\"system:serviceaccount:.*\\\" cannot get resource"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'SA cannot get resource' no Kubernetes?",
        "a": "'SA cannot get resource' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ServiceAccount not found",
    "slug": "serviceaccount-not-found",
    "aliases": [
      "ServiceAccount",
      "ServiceAccount not found",
      "found",
      "not"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ServiceAccount not found' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "serviceaccounts \\\".*\\\" not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ServiceAccount not found' no Kubernetes?",
        "a": "'ServiceAccount not found' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Unauthorized",
    "slug": "unauthorized",
    "aliases": [
      "Unauthorized"
    ],
    "category": "RBAC/Auth",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Unauthorized' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "Unauthorized",
        "re:the server responded with the status code 401"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Unauthorized' no Kubernetes?",
        "a": "'Unauthorized' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "NodeNotReady",
    "slug": "nodenotready",
    "aliases": [
      "NodeNotReady"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'NodeNotReady' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NodeNotReady",
        "node is not ready"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Libere recursos (disco/memória), reponha nós ruins e ajuste *eviction thresholds*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'NodeNotReady' no Kubernetes?",
        "a": "'NodeNotReady' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node MemoryPressure",
    "slug": "node-memorypressure",
    "aliases": [
      "MemoryPressure",
      "Node",
      "Node MemoryPressure",
      "NodeHasInsufficientMemory"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node MemoryPressure' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NodeHasInsufficientMemory",
        "Node has condition: MemoryPressure"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Libere recursos (disco/memória), reponha nós ruins e ajuste *eviction thresholds*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node MemoryPressure' no Kubernetes?",
        "a": "'Node MemoryPressure' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node DiskPressure",
    "slug": "node-diskpressure",
    "aliases": [
      "DiskPressure",
      "Node",
      "Node DiskPressure",
      "NodeHasDiskPressure"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node DiskPressure' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NodeHasDiskPressure",
        "Node has condition: DiskPressure"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Libere recursos (disco/memória), reponha nós ruins e ajuste *eviction thresholds*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node DiskPressure' no Kubernetes?",
        "a": "'Node DiskPressure' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Kubelet ContainerManager failure",
    "slug": "kubelet-containermanager-failure",
    "aliases": [
      "ContainerManager",
      "Kubelet",
      "Kubelet ContainerManager failure",
      "cgroup driver mismatch",
      "failure"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Kubelet ContainerManager failure' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "kubelet: failed to start ContainerManager",
        "cgroup driver mismatch"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Kubelet ContainerManager failure' no Kubernetes?",
        "a": "'Kubelet ContainerManager failure' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "failed to get metrics for resource",
    "slug": "failed-to-get-metrics-for-resource",
    "aliases": [
      "failed",
      "failed to get metrics for resource",
      "for",
      "get",
      "metrics",
      "resource"
    ],
    "category": "Observability",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'failed to get metrics for resource' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "failed to get metrics for resource",
        "no metrics returned from resource metrics API"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'failed to get metrics for resource' no Kubernetes?",
        "a": "'failed to get metrics for resource' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "HPA unable to compute replicas",
    "slug": "hpa-unable-to-compute-replicas",
    "aliases": [
      "HPA",
      "HPA unable to compute replicas",
      "compute",
      "replicas",
      "unable"
    ],
    "category": "Observability",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'HPA unable to compute replicas' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "HPA was unable to compute the replica count"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'HPA unable to compute replicas' no Kubernetes?",
        "a": "'HPA unable to compute replicas' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ClusterAutoscaler not provisioning nodes",
    "slug": "clusterautoscaler-not-provisioning-nodes",
    "aliases": [
      "ClusterAutoscaler",
      "ClusterAutoscaler not provisioning nodes",
      "NoNodeGroup",
      "NotTriggerScaleUp",
      "nodes",
      "not",
      "provisioning"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ClusterAutoscaler not provisioning nodes' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NoNodeGroup",
        "NotTriggerScaleUp",
        "max node group size reached"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'ClusterAutoscaler not provisioning nodes' no Kubernetes?",
        "a": "'ClusterAutoscaler not provisioning nodes' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Helm immutable field",
    "slug": "helm-immutable-field",
    "aliases": [
      "Helm",
      "Helm immutable field",
      "field",
      "immutable"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Helm immutable field' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "UPGRADE FAILED: cannot patch .* Invalid value: field is immutable"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Helm immutable field' no Kubernetes?",
        "a": "'Helm immutable field' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Helm resource already exists",
    "slug": "helm-resource-already-exists",
    "aliases": [
      "Helm",
      "Helm resource already exists",
      "already",
      "exists",
      "resource"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Helm resource already exists' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "INSTALLATION FAILED: rendered manifests contain a resource that already exists"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Helm resource already exists' no Kubernetes?",
        "a": "'Helm resource already exists' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "helm rollback failed",
    "slug": "helm-rollback-failed",
    "aliases": [
      "Error: rollback.* failed",
      "failed",
      "helm",
      "helm rollback failed",
      "no deployed releases",
      "rollback"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'helm rollback failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Error: rollback.* failed",
        "no deployed releases"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'helm rollback failed' no Kubernetes?",
        "a": "'helm rollback failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Prometheus scrape timeout",
    "slug": "prometheus-scrape-timeout",
    "aliases": [
      "Error scraping target",
      "Prometheus",
      "Prometheus scrape timeout",
      "scrape",
      "timeout"
    ],
    "category": "Observability",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Prometheus scrape timeout' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "prometheus scrape failed: context deadline exceeded",
        "Error scraping target"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Não aplicável; foco é rede."
      ],
      "corrections": [
        "Criar/ajustar Service e selectors; garantir endpoints prontos.",
        "Abrir NetworkPolicies para o fluxo necessário.",
        "Conferir portas/alvos corretos (targetPort/port).",
        "Configurar rotas/firewall para egress."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Prometheus scrape timeout' no Kubernetes?",
        "a": "'Prometheus scrape timeout' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Prometheus config reload error",
    "slug": "prometheus-config-reload-error",
    "aliases": [
      "Error reloading config",
      "Prometheus",
      "Prometheus config reload error",
      "config",
      "error",
      "reload"
    ],
    "category": "Observability",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Prometheus config reload error' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "Error reloading config",
        "re:couldn't load configuration .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Prometheus config reload error' no Kubernetes?",
        "a": "'Prometheus config reload error' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Grafana datasource not working",
    "slug": "grafana-datasource-not-working",
    "aliases": [
      "Grafana",
      "Grafana datasource not working",
      "datasource",
      "not",
      "working"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Grafana datasource not working' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Datasource is not working",
        "Invalid response code from datasource"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Grafana datasource not working' no Kubernetes?",
        "a": "'Grafana datasource not working' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Fluentd permission denied",
    "slug": "fluentd-permission-denied",
    "aliases": [
      "Fluentd",
      "Fluentd permission denied",
      "denied",
      "permission"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Fluentd permission denied' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "Fluentd permission denied",
        "re:tail .* permission denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Fluentd permission denied' no Kubernetes?",
        "a": "'Fluentd permission denied' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "No space left on device",
    "slug": "no-space-left-on-device",
    "aliases": [
      "ENOSPC",
      "No space left on device",
      "device",
      "left",
      "space"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'No space left on device' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "No space left on device",
        "ENOSPC"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'No space left on device' no Kubernetes?",
        "a": "'No space left on device' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Failed to connect to Elasticsearch",
    "slug": "failed-to-connect-to-elasticsearch",
    "aliases": [
      "Connection refused.*elasticsearch",
      "Elasticsearch",
      "Failed",
      "Failed to connect to Elasticsearch",
      "connect"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Failed to connect to Elasticsearch' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Failed to connect to Elasticsearch",
        "Connection refused.*elasticsearch"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Não aplicável; foco é rede."
      ],
      "corrections": [
        "Criar/ajustar Service e selectors; garantir endpoints prontos.",
        "Abrir NetworkPolicies para o fluxo necessário.",
        "Conferir portas/alvos corretos (targetPort/port).",
        "Configurar rotas/firewall para egress."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Failed to connect to Elasticsearch' no Kubernetes?",
        "a": "'Failed to connect to Elasticsearch' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "kubeadm init preflight failed",
    "slug": "kubeadm-init-preflight-failed",
    "aliases": [
      "failed",
      "init",
      "kubeadm",
      "kubeadm init preflight failed",
      "preflight",
      "re:\\[ERROR SystemVerification\\]"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'kubeadm init preflight failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "kubeadm init fails: preflight checks failed",
        "re:\\[ERROR SystemVerification\\]"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'kubeadm init preflight failed' no Kubernetes?",
        "a": "'kubeadm init preflight failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "kubeadm upgrade apply fails",
    "slug": "kubeadm-upgrade-apply-fails",
    "aliases": [
      "apply",
      "fails",
      "kubeadm",
      "kubeadm upgrade apply fails",
      "upgrade"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'kubeadm upgrade apply fails' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "kubeadm upgrade apply fails",
        "re:errors during upgrade apply"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'kubeadm upgrade apply fails' no Kubernetes?",
        "a": "'kubeadm upgrade apply fails' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "kubectl localhost:8080 refused",
    "slug": "kubectl-localhost-8080-refused",
    "aliases": [
      "8080",
      "kubectl",
      "kubectl localhost:8080 refused",
      "localhost",
      "refused"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'kubectl localhost:8080 refused' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "the connection to the server localhost:8080 was refused",
        "re:You must be logged in to the server"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'kubectl localhost:8080 refused' no Kubernetes?",
        "a": "'kubectl localhost:8080 refused' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "etcd request timed out",
    "slug": "etcd-request-timed-out",
    "aliases": [
      "etcd",
      "etcd request timed out",
      "out",
      "request",
      "timed"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'etcd request timed out' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "etcdserver: request timed out"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'etcd request timed out' no Kubernetes?",
        "a": "'etcd request timed out' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "etcd database space exceeded",
    "slug": "etcd-database-space-exceeded",
    "aliases": [
      "database",
      "etcd",
      "etcd database space exceeded",
      "exceeded",
      "space"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'etcd database space exceeded' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "etcdserver: mvcc: database space exceeded"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'etcd database space exceeded' no Kubernetes?",
        "a": "'etcd database space exceeded' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "High API server latency",
    "slug": "high-api-server-latency",
    "aliases": [
      "API",
      "High",
      "High API server latency",
      "LongRunningRequest",
      "apiserver latency",
      "latency",
      "server"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'High API server latency' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "apiserver latency",
        "LongRunningRequest"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'High API server latency' no Kubernetes?",
        "a": "'High API server latency' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "API server down",
    "slug": "api-server-down",
    "aliases": [
      "API",
      "API server down",
      "apiserver is down",
      "down",
      "server"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'API server down' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "apiserver is down",
        "connection refused .* kube-apiserver"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'API server down' no Kubernetes?",
        "a": "'API server down' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "conntrack table is full",
    "slug": "conntrack-table-is-full",
    "aliases": [
      "conntrack",
      "conntrack table is full",
      "full",
      "nf_conntrack: table full",
      "table"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'conntrack table is full' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "conntrack table is full",
        "nf_conntrack: table full"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'conntrack table is full' no Kubernetes?",
        "a": "'conntrack table is full' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "kube-proxy not working",
    "slug": "kube-proxy-not-working",
    "aliases": [
      "iptables-save failed",
      "kube-proxy",
      "kube-proxy not working",
      "not",
      "working"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'kube-proxy not working' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "kube-proxy not working",
        "Failed to sync endpoint slices",
        "iptables-save failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'kube-proxy not working' no Kubernetes?",
        "a": "'kube-proxy not working' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Scheduler not placing pods",
    "slug": "scheduler-not-placing-pods",
    "aliases": [
      "FailedScheduling",
      "Scheduler",
      "Scheduler not placing pods",
      "not",
      "placing",
      "pods"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Scheduler not placing pods' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "FailedScheduling",
        "re:scheduler cache is out of sync"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Scheduler not placing pods' no Kubernetes?",
        "a": "'Scheduler not placing pods' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod stuck Pending",
    "slug": "pod-stuck-pending",
    "aliases": [
      "Pending",
      "Pod",
      "Pod stuck Pending",
      "PodScheduled.*False",
      "PodScheduled.*Unschedulable",
      "stuck"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod stuck Pending' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "PodScheduled.*False",
        "PodScheduled.*Unschedulable"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Requests de CPU/memória altos e sem capacidade livre.",
        "Taints no nó sem tolerations correspondentes.",
        "Afinidade/antiafinidade/TopologySpread inviáveis.",
        "PVC pendente/StorageClass inexistente."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e leia a razão de Unschedulable.",
        "Compare requests com capacidade: kubectl get nodes -o wide e métricas.",
        "Revise tolerations/affinity/topologySpreadConstraints.",
        "Se houver PVC, confira: kubectl get pvc,pv -n <ns>."
      ],
      "best_practices": [
        "Requests proporcionais ao uso real; limite overcommit.",
        "Padronize tolerations e use NodeAffinity com rótulos coerentes.",
        "Use autoscaling de nós quando cabível.",
        "Valide StorageClass/PVC nos ambientes."
      ],
      "storage_diagnostics": [
        "Para PVC: confirme StorageClass, quota e disponibilidade de PVs."
      ],
      "corrections": [
        "Reduza requests ou aumente capacidade/escale o cluster.",
        "Adicione tolerations correspondentes aos taints necessários.",
        "Simplifique affinities/topologySpread.",
        "Ajuste PVC/StorageClass para binding correto."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod stuck Pending' no Kubernetes?",
        "a": "'Pod stuck Pending' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Container cannot start",
    "slug": "container-cannot-start",
    "aliases": [
      "Container",
      "Container cannot start",
      "cannot",
      "start"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Container cannot start' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "ContainerCreating for too long",
        "re:Error: cannot start service"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Container cannot start' no Kubernetes?",
        "a": "'Container cannot start' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Invalid resource quantity",
    "slug": "invalid-resource-quantity",
    "aliases": [
      "Invalid",
      "Invalid resource quantity",
      "invalid resource quantity",
      "quantity",
      "resource"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Invalid resource quantity' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "invalid resource quantity",
        "re:quantities must be positive values"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Invalid resource quantity' no Kubernetes?",
        "a": "'Invalid resource quantity' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Immutable field changed",
    "slug": "immutable-field-changed",
    "aliases": [
      "Immutable",
      "Immutable field changed",
      "changed",
      "field",
      "field is immutable"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Immutable field changed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "field is immutable",
        "re:Invalid value: .* field is immutable"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Immutable field changed' no Kubernetes?",
        "a": "'Immutable field changed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Admission webhook denied",
    "slug": "admission-webhook-denied",
    "aliases": [
      "Admission",
      "Admission webhook denied",
      "denied",
      "webhook"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Admission webhook denied' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "denied by admission webhook",
        "re:admission webhook .* denied the request"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Admission webhook denied' no Kubernetes?",
        "a": "'Admission webhook denied' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Webhook connection failure",
    "slug": "webhook-connection-failure",
    "aliases": [
      "Webhook",
      "Webhook connection failure",
      "connection",
      "failure"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Webhook connection failure' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "re:failed calling webhook .*: Post .* dial tcp .*: connect: connection refused",
        "x509: certificate signed by unknown authority"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Webhook connection failure' no Kubernetes?",
        "a": "'Webhook connection failure' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PodSecurity admission denied",
    "slug": "podsecurity-admission-denied",
    "aliases": [
      "PodSecurity",
      "PodSecurity admission denied",
      "admission",
      "denied",
      "forbidden: violates PodSecurity",
      "violates PodSecurity"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'PodSecurity admission denied' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "violates PodSecurity",
        "forbidden: violates PodSecurity"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PodSecurity admission denied' no Kubernetes?",
        "a": "'PodSecurity admission denied' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ResourceQuota exceeded",
    "slug": "resourcequota-exceeded",
    "aliases": [
      "ResourceQuota",
      "ResourceQuota exceeded",
      "exceeded",
      "exceeded quota",
      "re:exceeded quota: .*"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ResourceQuota exceeded' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "exceeded quota",
        "re:exceeded quota: .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ResourceQuota exceeded' no Kubernetes?",
        "a": "'ResourceQuota exceeded' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "LimitRange violation",
    "slug": "limitrange-violation",
    "aliases": [
      "LimitRange",
      "LimitRange violation",
      "violation"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'LimitRange violation' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "must not exceed limit",
        "re:must be less than or equal to cpu limit"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'LimitRange violation' no Kubernetes?",
        "a": "'LimitRange violation' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Too many open files",
    "slug": "too-many-open-files",
    "aliases": [
      "EMFILE",
      "Too",
      "Too many open files",
      "files",
      "many",
      "open"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Too many open files' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "too many open files",
        "EMFILE"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Limite nofile muito baixo para o workload.",
        "Muitas conexões simultâneas/arquivos abertos sem fechamento."
      ],
      "immediate_actions": [
        "Confirmar erro 'too many open files' nos logs.",
        "Aumentar temporariamente limites via securityContext (se suportado) ou ajuste do contêiner base.",
        "Reduzir conexões e fechar descritores corretamente."
      ],
      "best_practices": [
        "Dimensionar nofile conforme o perfil de carga.",
        "Implementar pooling e reuso de conexões."
      ],
      "storage_diagnostics": [
        "Não aplicável."
      ],
      "corrections": [
        "Ajustar ulimit nofile e padrões do SO/conteiner.",
        "Revisar código para fechamento de FDs."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Too many open files' no Kubernetes?",
        "a": "'Too many open files' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Device or resource busy",
    "slug": "device-or-resource-busy",
    "aliases": [
      "Device",
      "Device or resource busy",
      "EBUSY",
      "busy",
      "resource"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Device or resource busy' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "device or resource busy",
        "EBUSY"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Device or resource busy' no Kubernetes?",
        "a": "'Device or resource busy' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Segmentation fault",
    "slug": "segmentation-fault",
    "aliases": [
      "SIGSEGV",
      "Segmentation",
      "Segmentation fault",
      "fault",
      "signal: segmentation fault"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Segmentation fault' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Segmentation fault",
        "signal: segmentation fault",
        "SIGSEGV"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Segmentation fault' no Kubernetes?",
        "a": "'Segmentation fault' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Java OutOfMemoryError",
    "slug": "java-outofmemoryerror",
    "aliases": [
      "Java",
      "Java OutOfMemoryError",
      "OutOfMemoryError",
      "java.lang.OutOfMemoryError"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Java OutOfMemoryError' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "java.lang.OutOfMemoryError",
        "GC overhead limit exceeded"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Java OutOfMemoryError' no Kubernetes?",
        "a": "'Java OutOfMemoryError' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Panic in Go",
    "slug": "panic-in-go",
    "aliases": [
      "Panic",
      "Panic in Go",
      "panic:"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Panic in Go' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "panic:",
        "runtime: out of memory"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Panic in Go' no Kubernetes?",
        "a": "'Panic in Go' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Back-off pulling image",
    "slug": "back-off-pulling-image",
    "aliases": [
      "Back-off",
      "Back-off pulling image",
      "back-off pulling image",
      "image",
      "pulling"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Back-off pulling image' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "back-off pulling image",
        "Error response from daemon: manifest unknown"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'Back-off pulling image' no Kubernetes?",
        "a": "'Back-off pulling image' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Secret not found",
    "slug": "secret-not-found",
    "aliases": [
      "Secret",
      "Secret not found",
      "found",
      "not"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Secret not found' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "secret .* not found",
        "re:secrets \\\".*\\\" not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Secret not found' no Kubernetes?",
        "a": "'Secret not found' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ConfigMap not found",
    "slug": "configmap-not-found",
    "aliases": [
      "ConfigMap",
      "ConfigMap not found",
      "found",
      "not"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ConfigMap not found' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "configmap .* not found",
        "re:configmaps \\\".*\\\" not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ConfigMap not found' no Kubernetes?",
        "a": "'ConfigMap not found' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Job deadline exceeded",
    "slug": "job-deadline-exceeded",
    "aliases": [
      "DeadlineExceeded",
      "Job",
      "Job deadline exceeded",
      "deadline",
      "exceeded"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Job deadline exceeded' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "DeadlineExceeded",
        "re:Job .* has reached the specified backoff limit"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Job deadline exceeded' no Kubernetes?",
        "a": "'Job deadline exceeded' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Image layer extraction failed",
    "slug": "image-layer-extraction-failed",
    "aliases": [
      "Image",
      "Image layer extraction failed",
      "extraction",
      "failed",
      "layer"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Image layer extraction failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "failed to register layer",
        "re:error pulling image configuration.*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Image layer extraction failed' no Kubernetes?",
        "a": "'Image layer extraction failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "TLS: handshake timeout",
    "slug": "tls-handshake-timeout",
    "aliases": [
      "TLS",
      "TLS: handshake timeout",
      "handshake",
      "handshake timeout",
      "timeout"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'TLS: handshake timeout' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "handshake timeout",
        "Client.Timeout exceeded while awaiting headers"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'TLS: handshake timeout' no Kubernetes?",
        "a": "'TLS: handshake timeout' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node clock skew",
    "slug": "node-clock-skew",
    "aliases": [
      "Node",
      "Node clock skew",
      "clock",
      "skew"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node clock skew' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "x509: certificate has expired or is not yet valid",
        "certificate not yet valid"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node clock skew' no Kubernetes?",
        "a": "'Node clock skew' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod evicted: memory",
    "slug": "pod-evicted-memory",
    "aliases": [
      "Pod",
      "Pod evicted: memory",
      "evicted",
      "memory"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod evicted: memory' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "evicted: The node was low on resource: memory",
        "Evicted: The node had condition: MemoryPressure"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Pressão de memória/disk no nó.",
        "Requests/limits mal dimensionados levando a pressão.",
        "Volumes temporários enchendo o disco (emptyDir/logs)."
      ],
      "immediate_actions": [
        "Descrever pod para motivo de Eviction.",
        "kubectl top node/pod para ver consumo.",
        "Liberar espaço em /var/lib/docker/containerd e volumes temporários.",
        "Rebalancear workloads ou escalar nós."
      ],
      "best_practices": [
        "Definir requests/limits e limites de logs/emptyDir (sizeLimit).",
        "Coletar/rotacionar logs (stdout/stderr + agregador).",
        "Monitorar DiskPressure/MemoryPressure e agir proativamente."
      ],
      "storage_diagnostics": [
        "Checar espaço em disco do nó e volumes temporários."
      ],
      "corrections": [
        "Aumentar capacidade/escala; reduzir consumo por pod.",
        "Limitar size de emptyDir e rodar limpeza de logs.",
        "Revisar quotas e políticas de retenção."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod evicted: memory' no Kubernetes?",
        "a": "'Pod evicted: memory' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod evicted: disk",
    "slug": "pod-evicted-disk",
    "aliases": [
      "DiskPressure",
      "Pod",
      "Pod evicted: disk",
      "disk",
      "evicted"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod evicted: disk' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "evicted: The node was low on resource: ephemeral-storage",
        "DiskPressure"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Pressão de memória/disk no nó.",
        "Requests/limits mal dimensionados levando a pressão.",
        "Volumes temporários enchendo o disco (emptyDir/logs)."
      ],
      "immediate_actions": [
        "Descrever pod para motivo de Eviction.",
        "kubectl top node/pod para ver consumo.",
        "Liberar espaço em /var/lib/docker/containerd e volumes temporários.",
        "Rebalancear workloads ou escalar nós."
      ],
      "best_practices": [
        "Definir requests/limits e limites de logs/emptyDir (sizeLimit).",
        "Coletar/rotacionar logs (stdout/stderr + agregador).",
        "Monitorar DiskPressure/MemoryPressure e agir proativamente."
      ],
      "storage_diagnostics": [
        "Checar espaço em disco do nó e volumes temporários."
      ],
      "corrections": [
        "Aumentar capacidade/escala; reduzir consumo por pod.",
        "Limitar size de emptyDir e rodar limpeza de logs.",
        "Revisar quotas e políticas de retenção."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod evicted: disk' no Kubernetes?",
        "a": "'Pod evicted: disk' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ImagePull rate limit",
    "slug": "imagepull-rate-limit",
    "aliases": [
      "ImagePull",
      "ImagePull rate limit",
      "limit",
      "rate",
      "toomanyrequests: Rate exceeded"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ImagePull rate limit' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "toomanyrequests: Rate exceeded"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'ImagePull rate limit' no Kubernetes?",
        "a": "'ImagePull rate limit' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node filesystem read-only",
    "slug": "node-filesystem-read-only",
    "aliases": [
      "Node",
      "Node filesystem read-only",
      "filesystem",
      "read-only"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node filesystem read-only' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "Read-only file system on node",
        "overlayfs: failed to mount"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node filesystem read-only' no Kubernetes?",
        "a": "'Node filesystem read-only' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Kubelet PLEG not healthy",
    "slug": "kubelet-pleg-not-healthy",
    "aliases": [
      "Kubelet",
      "Kubelet PLEG not healthy",
      "PLEG",
      "healthy",
      "not"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Kubelet PLEG not healthy' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "PLEG is not healthy"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Kubelet PLEG not healthy' no Kubernetes?",
        "a": "'Kubelet PLEG not healthy' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod blocked by NetworkPolicy",
    "slug": "pod-blocked-by-networkpolicy",
    "aliases": [
      "NetworkPolicy",
      "Pod",
      "Pod blocked by NetworkPolicy",
      "blocked",
      "networkpolicy denied"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod blocked by NetworkPolicy' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "networkpolicy denied",
        "Connection timed out with NetworkPolicy"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod blocked by NetworkPolicy' no Kubernetes?",
        "a": "'Pod blocked by NetworkPolicy' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Service type NodePort blocked",
    "slug": "service-type-nodeport-blocked",
    "aliases": [
      "ECONNREFUSED to <nodeIP>:<nodePort>",
      "NodePort",
      "NodePort service inaccessible",
      "Service",
      "Service type NodePort blocked",
      "blocked",
      "type"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Service type NodePort blocked' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NodePort service inaccessible",
        "ECONNREFUSED to <nodeIP>:<nodePort>"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Service type NodePort blocked' no Kubernetes?",
        "a": "'Service type NodePort blocked' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod priority preempted",
    "slug": "pod-priority-preempted",
    "aliases": [
      "Pod",
      "Pod priority preempted",
      "Preempted",
      "preempted",
      "priority"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod priority preempted' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Preempted",
        "Preempted by higher priority pod"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod priority preempted' no Kubernetes?",
        "a": "'Pod priority preempted' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "HostPath permission denied",
    "slug": "hostpath-permission-denied",
    "aliases": [
      "HostPath",
      "HostPath permission denied",
      "denied",
      "permission"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'HostPath permission denied' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "hostPath .* permission denied",
        "re:cannot mount \\\"hostPath\\\".* permission denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'HostPath permission denied' no Kubernetes?",
        "a": "'HostPath permission denied' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CSI driver not installed",
    "slug": "csi-driver-not-installed",
    "aliases": [
      "CSI",
      "CSI driver not installed",
      "driver",
      "installed",
      "not"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'CSI driver not installed' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "failed to find a persistentvolumeclaim to match",
        "CSI driver not found",
        "no volume plugin matched"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CSI driver not installed' no Kubernetes?",
        "a": "'CSI driver not installed' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CSI attachment timeout",
    "slug": "csi-attachment-timeout",
    "aliases": [
      "AttachVolume.* timed out",
      "CSI",
      "CSI attachment timeout",
      "attachment",
      "timeout"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CSI attachment timeout' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "timed out waiting for the condition while mounting volume",
        "AttachVolume.* timed out"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CSI attachment timeout' no Kubernetes?",
        "a": "'CSI attachment timeout' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod uses hostNetwork without permissions",
    "slug": "pod-uses-hostnetwork-without-permissions",
    "aliases": [
      "Pod",
      "Pod uses hostNetwork without permissions",
      "hostNetwork",
      "permissions",
      "uses",
      "without"
    ],
    "category": "RBAC/Auth",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod uses hostNetwork without permissions' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "forbidden: use of host network is not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod uses hostNetwork without permissions' no Kubernetes?",
        "a": "'Pod uses hostNetwork without permissions' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod uses hostPID/hostIPC denied",
    "slug": "pod-uses-hostpid-hostipc-denied",
    "aliases": [
      "Pod",
      "Pod uses hostPID/hostIPC denied",
      "denied",
      "hostPID/hostIPC",
      "uses"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Pod uses hostPID/hostIPC denied' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "forbidden: use of host pid is not allowed",
        "forbidden: use of host ipc is not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod uses hostPID/hostIPC denied' no Kubernetes?",
        "a": "'Pod uses hostPID/hostIPC denied' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Capabilities denied",
    "slug": "capabilities-denied",
    "aliases": [
      "Capabilities",
      "Capabilities denied",
      "cap_sys_admin not allowed",
      "denied"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Capabilities denied' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "forbidden: capability .* is not allowed",
        "cap_sys_admin not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Capabilities denied' no Kubernetes?",
        "a": "'Capabilities denied' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "RunAsUser forbidden",
    "slug": "runasuser-forbidden",
    "aliases": [
      "RunAsUser",
      "RunAsUser forbidden",
      "forbidden",
      "forbidden: must runAsNonRoot",
      "runAsUser is forbidden"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'RunAsUser forbidden' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "forbidden: must runAsNonRoot",
        "runAsUser is forbidden"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'RunAsUser forbidden' no Kubernetes?",
        "a": "'RunAsUser forbidden' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "HostPort denied",
    "slug": "hostport-denied",
    "aliases": [
      "HostPort",
      "HostPort denied",
      "denied"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'HostPort denied' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "forbidden: host ports are not allowed",
        "HostPort is not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'HostPort denied' no Kubernetes?",
        "a": "'HostPort denied' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Privileged container forbidden",
    "slug": "privileged-container-forbidden",
    "aliases": [
      "Privileged",
      "Privileged container forbidden",
      "container",
      "forbidden"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Privileged container forbidden' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "forbidden: privileged containers are not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Privileged container forbidden' no Kubernetes?",
        "a": "'Privileged container forbidden' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Sysctl forbidden",
    "slug": "sysctl-forbidden",
    "aliases": [
      "Sysctl",
      "Sysctl forbidden",
      "forbidden",
      "forbidden sysctl"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Sysctl forbidden' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "forbidden sysctl",
        "unsafe sysctl not allowed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Sysctl forbidden' no Kubernetes?",
        "a": "'Sysctl forbidden' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Downward API reference invalid",
    "slug": "downward-api-reference-invalid",
    "aliases": [
      "API",
      "Downward",
      "Downward API reference invalid",
      "fieldRef.* not supported",
      "invalid",
      "reference",
      "status.podIP not found"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Downward API reference invalid' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "fieldRef.* not supported",
        "status.podIP not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Downward API reference invalid' no Kubernetes?",
        "a": "'Downward API reference invalid' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PodAntiAffinity prevents scheduling",
    "slug": "podantiaffinity-prevents-scheduling",
    "aliases": [
      "PodAntiAffinity",
      "PodAntiAffinity prevents scheduling",
      "matchExpressions.* prevents matching",
      "prevents",
      "requiredDuringSchedulingIgnoredDuringExecution",
      "scheduling"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'PodAntiAffinity prevents scheduling' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "requiredDuringSchedulingIgnoredDuringExecution",
        "matchExpressions.* prevents matching"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PodAntiAffinity prevents scheduling' no Kubernetes?",
        "a": "'PodAntiAffinity prevents scheduling' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Topology spread constraints blocked",
    "slug": "topology-spread-constraints-blocked",
    "aliases": [
      "Topology",
      "Topology spread constraints blocked",
      "blocked",
      "constraints",
      "matchLabelKeys.* not satisfied",
      "spread",
      "topologySpreadConstraints"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Topology spread constraints blocked' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "matchLabelKeys.* not satisfied",
        "topologySpreadConstraints"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Topology spread constraints blocked' no Kubernetes?",
        "a": "'Topology spread constraints blocked' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Service external IP pending",
    "slug": "service-external-ip-pending",
    "aliases": [
      "Service",
      "Service external IP pending",
      "external",
      "pending",
      "pending external IP",
      "type LoadBalancer.* pending"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Service external IP pending' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "pending external IP",
        "type LoadBalancer.* pending"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Requests de CPU/memória altos e sem capacidade livre.",
        "Taints no nó sem tolerations correspondentes.",
        "Afinidade/antiafinidade/TopologySpread inviáveis.",
        "PVC pendente/StorageClass inexistente."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e leia a razão de Unschedulable.",
        "Compare requests com capacidade: kubectl get nodes -o wide e métricas.",
        "Revise tolerations/affinity/topologySpreadConstraints.",
        "Se houver PVC, confira: kubectl get pvc,pv -n <ns>."
      ],
      "best_practices": [
        "Requests proporcionais ao uso real; limite overcommit.",
        "Padronize tolerations e use NodeAffinity com rótulos coerentes.",
        "Use autoscaling de nós quando cabível.",
        "Valide StorageClass/PVC nos ambientes."
      ],
      "storage_diagnostics": [
        "Para PVC: confirme StorageClass, quota e disponibilidade de PVs."
      ],
      "corrections": [
        "Reduza requests ou aumente capacidade/escale o cluster.",
        "Adicione tolerations correspondentes aos taints necessários.",
        "Simplifique affinities/topologySpread.",
        "Ajuste PVC/StorageClass para binding correto."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Service external IP pending' no Kubernetes?",
        "a": "'Service external IP pending' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "API rate limit exceeded",
    "slug": "api-rate-limit-exceeded",
    "aliases": [
      "API",
      "API rate limit exceeded",
      "Too Many Requests",
      "exceeded",
      "limit",
      "rate"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'API rate limit exceeded' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Too Many Requests",
        "re:throttling request .* due to client-side throttling"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'API rate limit exceeded' no Kubernetes?",
        "a": "'API rate limit exceeded' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod stuck Terminating",
    "slug": "pod-stuck-terminating",
    "aliases": [
      "Orphaned pod",
      "Pod",
      "Pod stuck Terminating",
      "Terminating",
      "error killing pod",
      "finalizers prevent deletion",
      "stuck"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod stuck Terminating' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Pod .* is terminating",
        "finalizers prevent deletion",
        "error killing pod",
        "Orphaned pod",
        "Terminating"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod stuck Terminating' no Kubernetes?",
        "a": "'Pod stuck Terminating' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Image pull unauthorized",
    "slug": "image-pull-unauthorized",
    "aliases": [
      "Image",
      "Image pull unauthorized",
      "pull",
      "unauthorized",
      "unauthorized: authentication required"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Image pull unauthorized' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "unauthorized: authentication required",
        "401 Unauthorized from registry"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Use tags imutáveis (SHA digest) e política imagePullPolicy adequada.",
        "Automatize login/secret via controlador (Workload Identity/IRSA).",
        "Evite 'latest'; versionamento semântico ajuda rollback.",
        "Monitore taxa de erro de pull."
      ],
      "storage_diagnostics": [
        "Não aplicável diretamente; apenas confirme espaço em disco do nó para camadas da imagem."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'Image pull unauthorized' no Kubernetes?",
        "a": "'Image pull unauthorized' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Image manifest unknown",
    "slug": "image-manifest-unknown",
    "aliases": [
      "Image",
      "Image manifest unknown",
      "manifest",
      "manifest not found",
      "manifest unknown",
      "unknown"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Image manifest unknown' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "manifest unknown",
        "manifest not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome da imagem/tag incorretos ou digest inexistente.",
        "Credenciais ausentes/secret de pull errado.",
        "Permissões no registry/GCR/ECR/AzureCR insuficientes.",
        "Bloqueio de egress/rota/firewall até o registry."
      ],
      "immediate_actions": [
        "Ver eventos do pod (ErrImagePull/ImagePullBackOff).",
        "Confirme a imagem: docker pull / crictl pull no nó.",
        "Valide imagePullSecrets e permissões no registry.",
        "Teste conectividade ao registry a partir do nó/pod utilitário."
      ],
      "best_practices": [
        "Use tags imutáveis (SHA digest) e política imagePullPolicy adequada.",
        "Automatize login/secret via controlador (Workload Identity/IRSA).",
        "Evite 'latest'; versionamento semântico ajuda rollback.",
        "Monitore taxa de erro de pull."
      ],
      "storage_diagnostics": [
        "Não aplicável diretamente; apenas confirme espaço em disco do nó para camadas da imagem."
      ],
      "corrections": [
        "Corrija nome/tag/digest e publique a imagem no registry correto.",
        "Configure/associe imagePullSecrets válidos.",
        "Abra egress necessário para o registry.",
        "Defina imagePullPolicy: IfNotPresent quando adequado para reduzir pulls."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Image manifest unknown' no Kubernetes?",
        "a": "'Image manifest unknown' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Invalid mount path",
    "slug": "invalid-mount-path",
    "aliases": [
      "Invalid",
      "Invalid mount path",
      "invalid mount path",
      "is not absolute",
      "mount",
      "path"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Invalid mount path' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "is not absolute",
        "invalid mount path"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Credenciais/secret do CSI inválidos.",
        "Permissões de filesystem (UID/GID) incompatíveis.",
        "AccessMode incompatível (ex.: ReadWriteOnce em múltiplos pods).",
        "Path de montagem incorreto ou readOnly inesperado."
      ],
      "immediate_actions": [
        "Descreva o pod e o PVC/PV para eventos do CSI.",
        "Confirme fsGroup/runAsUser e permissões do path.",
        "Valide secrets/params do volume (ex.: NFS, S3, Filestore).",
        "Teste montagem com pod utilitário (busybox/alpine)."
      ],
      "best_practices": [
        "Padronize fsGroup e permissões.",
        "Documente AccessModes por tipo de workload.",
        "Evite execução como root quando possível; use SecurityContext.",
        "Monitore erros do driver CSI."
      ],
      "storage_diagnostics": [
        "Logs do driver CSI e eventos do kubelet relativos ao volume."
      ],
      "corrections": [
        "Corrigir fsGroup/UID/GID e permissões do diretório.",
        "Usar AccessMode compatível com o padrão de uso.",
        "Ajustar secrets/parâmetros do volume.",
        "Conferir flags readOnly e mountPath no spec."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Invalid mount path' no Kubernetes?",
        "a": "'Invalid mount path' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PVC access mode mismatch",
    "slug": "pvc-access-mode-mismatch",
    "aliases": [
      "PVC",
      "PVC access mode mismatch",
      "access",
      "mismatch",
      "mode"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'PVC access mode mismatch' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "does not support requested access mode",
        "re:PV .* does not support accessModes"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "StorageClass ausente/errada ou provisioner indisponível.",
        "Requisitos de acesso/tamanho/selector incompatíveis com PVs.",
        "Zonas/afinidade de volume não casam com o nó.",
        "Cotas de storage esgotadas."
      ],
      "immediate_actions": [
        "kubectl describe pvc <pvc> -n <ns> e confira eventos.",
        "kubectl get sc,pv -A e verifique disponibilidade/labels.",
        "Ajuste requests de storage e accessModes.",
        "Confirme zone/allowedTopologies/NodeAffinity do PV."
      ],
      "best_practices": [
        "Use StorageClass padrão e tamanhos padronizados.",
        "Evite selectors desnecessários; prefira dinâmico.",
        "Monitore binding time e erros do provisioner.",
        "Defina quotas/limites por namespace."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PVC access mode mismatch' no Kubernetes?",
        "a": "'PVC access mode mismatch' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ReadWriteMany not supported",
    "slug": "readwritemany-not-supported",
    "aliases": [
      "ReadWriteMany",
      "ReadWriteMany not supported",
      "not",
      "supported"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ReadWriteMany not supported' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "provisioner does not support ReadWriteMany"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ReadWriteMany not supported' no Kubernetes?",
        "a": "'ReadWriteMany not supported' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PodSecurityContext fsGroup invalid",
    "slug": "podsecuritycontext-fsgroup-invalid",
    "aliases": [
      "PodSecurityContext",
      "PodSecurityContext fsGroup invalid",
      "fsGroup",
      "invalid"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'PodSecurityContext fsGroup invalid' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "fsGroup is not allowed",
        "fsGroup must be in the ranges"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PodSecurityContext fsGroup invalid' no Kubernetes?",
        "a": "'PodSecurityContext fsGroup invalid' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node affinity unsatisfiable",
    "slug": "node-affinity-unsatisfiable",
    "aliases": [
      "Node",
      "Node affinity unsatisfiable",
      "RequiredDuringScheduling.* not matched",
      "affinity",
      "unsatisfiable"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node affinity unsatisfiable' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "node(s) didn't match Pod's node affinity",
        "RequiredDuringScheduling.* not matched"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node affinity unsatisfiable' no Kubernetes?",
        "a": "'Node affinity unsatisfiable' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Image pull network unreachable",
    "slug": "image-pull-network-unreachable",
    "aliases": [
      "Image",
      "Image pull network unreachable",
      "network",
      "pull",
      "unreachable"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Image pull network unreachable' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "Temporary failure in name resolution",
        "no route to host"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'Image pull network unreachable' no Kubernetes?",
        "a": "'Image pull network unreachable' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CoreDNS CrashLoop",
    "slug": "coredns-crashloop",
    "aliases": [
      "CoreDNS",
      "CoreDNS CrashLoop",
      "CrashLoop",
      "coredns.* CrashLoopBackOff"
    ],
    "category": "Networking/DNS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "coredns.* CrashLoopBackOff",
        "plugin/loop: seen more than 5 dns requests with same id"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falha no entrypoint/command, dependência externa indisponível, config ausente, ou panic da aplicação.",
        "Probes agressivas derrubando o processo durante a inicialização."
      ],
      "immediate_actions": [
        "Veja o contêiner anterior: kubectl logs <pod> -n <ns> --previous (indica a causa de saída).",
        "Descreva o pod e eventos: kubectl describe pod <pod> -n <ns> (erros de mount/env/args)."
      ],
      "best_practices": [
        "Valide variáveis/Secrets/ConfigMaps antes do deploy.",
        "Use startupProbe para apps pesadas; dê tempo para aquecimento.",
        "Padronize healthchecks e exponha métricas para observabilidade.",
        "Mantenha logs estruturados para facilitar triagem."
      ],
      "storage_diagnostics": [
        "Se envolver volumes, verifique permissões, paths e montagem antes do processo iniciar."
      ],
      "corrections": [
        "Corrija env/args/arquivos requeridos; torne scripts executáveis (chmod +x).",
        "Se devido a readiness/liveness, relaxe initialDelaySeconds, periodSeconds, timeoutSeconds e failureThreshold.",
        "Aumente terminationGracePeriodSeconds se precisar finalizar com graceful shutdown."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `command`/`args`/`entrypoint`; torne scripts executáveis.",
      "Revise *probes* e aumente tempos/thresholds conforme necessidade.",
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CoreDNS CrashLoop' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CoreDNS config error",
    "slug": "coredns-config-error",
    "aliases": [
      "CoreDNS",
      "CoreDNS config error",
      "config",
      "error"
    ],
    "category": "Networking/DNS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "Failed to start CoreDNS",
        "Error loading configuration: .*"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CoreDNS config error' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Kubelet certificate issue",
    "slug": "kubelet-certificate-issue",
    "aliases": [
      "Kubelet",
      "Kubelet certificate issue",
      "certificate",
      "issue"
    ],
    "category": "Certificates/ACME",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Kubelet certificate issue' geralmente indica um problema na categoria Certificates/ACME",
    "signals": {
      "log_keywords": [
        "failed to verify certificate",
        "x509: certificate signed by unknown authority"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Kubelet certificate issue' no Kubernetes?",
        "a": "'Kubelet certificate issue' geralmente indica um problema na categoria Certificates/ACME"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "APIServer authorization error",
    "slug": "apiserver-authorization-error",
    "aliases": [
      "APIServer",
      "APIServer authorization error",
      "Forbidden (user=system:serviceaccount",
      "authorization",
      "error",
      "re:RBAC: access denied"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'APIServer authorization error' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Forbidden (user=system:serviceaccount",
        "re:RBAC: access denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'APIServer authorization error' no Kubernetes?",
        "a": "'APIServer authorization error' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Mutating webhook timeout",
    "slug": "mutating-webhook-timeout",
    "aliases": [
      "Mutating",
      "Mutating webhook timeout",
      "timeout",
      "webhook"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Mutating webhook timeout' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "admission webhook .* did not respond",
        "re:context deadline exceeded.* calling webhook"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Service/Endpoints inexistentes ou apontando para pods não prontos.",
        "NetworkPolicy bloqueando tráfego entre pods/namespaces.",
        "Portas erradas, readiness falhando ou headless service mal configurado.",
        "Rota/Firewall/VPC impedindo saída."
      ],
      "immediate_actions": [
        "kubectl get svc,ep -n <ns> e verifique endpoints.",
        "kubectl exec -it <pod> -- curl -v http://<svc>:<port> (ou nc).",
        "Revisar NetworkPolicies para permitir o tráfego exigido.",
        "Checar readiness e portas no Deployment/Service."
      ],
      "best_practices": [
        "Padronize portas e nomes de services.",
        "Use probes e labels consistentes para selectors.",
        "Documente dependências externas e permita egress necessário.",
        "Monitore latência/erros por service."
      ],
      "storage_diagnostics": [
        "Não aplicável; foco é rede."
      ],
      "corrections": [
        "Criar/ajustar Service e selectors; garantir endpoints prontos.",
        "Abrir NetworkPolicies para o fluxo necessário.",
        "Conferir portas/alvos corretos (targetPort/port).",
        "Configurar rotas/firewall para egress."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Mutating webhook timeout' no Kubernetes?",
        "a": "'Mutating webhook timeout' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Long image build tag mismatch",
    "slug": "long-image-build-tag-mismatch",
    "aliases": [
      "ImagePullBackOff .* latest",
      "Long",
      "Long image build tag mismatch",
      "build",
      "image",
      "mismatch",
      "tag"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Long image build tag mismatch' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "ImagePullBackOff .* latest",
        "Always pull with 'latest'"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome da imagem/tag incorretos ou digest inexistente.",
        "Credenciais ausentes/secret de pull errado.",
        "Permissões no registry/GCR/ECR/AzureCR insuficientes.",
        "Bloqueio de egress/rota/firewall até o registry."
      ],
      "immediate_actions": [
        "Ver eventos do pod (ErrImagePull/ImagePullBackOff).",
        "Confirme a imagem: docker pull / crictl pull no nó.",
        "Valide imagePullSecrets e permissões no registry.",
        "Teste conectividade ao registry a partir do nó/pod utilitário."
      ],
      "best_practices": [
        "Use tags imutáveis (SHA digest) e política imagePullPolicy adequada.",
        "Automatize login/secret via controlador (Workload Identity/IRSA).",
        "Evite 'latest'; versionamento semântico ajuda rollback.",
        "Monitore taxa de erro de pull."
      ],
      "storage_diagnostics": [
        "Não aplicável diretamente; apenas confirme espaço em disco do nó para camadas da imagem."
      ],
      "corrections": [
        "Corrija nome/tag/digest e publique a imagem no registry correto.",
        "Configure/associe imagePullSecrets válidos.",
        "Abra egress necessário para o registry.",
        "Defina imagePullPolicy: IfNotPresent quando adequado para reduzir pulls."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Long image build tag mismatch' no Kubernetes?",
        "a": "'Long image build tag mismatch' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Forbidden hostPath /var/run/docker.sock",
    "slug": "forbidden-hostpath-var-run-docker-sock",
    "aliases": [
      "/var/run/docker",
      "/var/run/docker.sock is forbidden",
      "Forbidden",
      "Forbidden hostPath /var/run/docker.sock",
      "hostPath",
      "sock"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Forbidden hostPath /var/run/docker.sock' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "/var/run/docker.sock is forbidden",
        "hostPath .* docker.sock .* forbidden"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Forbidden hostPath /var/run/docker.sock' no Kubernetes?",
        "a": "'Forbidden hostPath /var/run/docker.sock' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Ephemeral storage exceeded",
    "slug": "ephemeral-storage-exceeded",
    "aliases": [
      "Ephemeral",
      "Ephemeral storage exceeded",
      "disk usage exceeded",
      "exceeded",
      "storage"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Ephemeral storage exceeded' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "evicted: The node was low on resource: ephemeral-storage",
        "disk usage exceeded"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Ephemeral storage exceeded' no Kubernetes?",
        "a": "'Ephemeral storage exceeded' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod DNS policy misconfigured",
    "slug": "pod-dns-policy-misconfigured",
    "aliases": [
      "DNS",
      "Pod",
      "Pod DNS policy misconfigured",
      "misconfigured",
      "nameserver .* refused",
      "policy"
    ],
    "category": "Networking/DNS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "nameserver .* refused",
        "reply from unexpected source"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FQDN/hostname incorreto ou sem sufixo de busca (.svc.cluster.local).",
        "Service inexistente ou sem endpoints (selectors não batem).",
        "CoreDNS com ConfigMap inválido (forward/stubDomains) ou pods em CrashLoop/sem recursos.",
        "NetworkPolicy bloqueando egress UDP/TCP 53 para kube-dns.",
        "Resolver upstream fora do ar/sem rota (firewall/VPC/peering/proxy).",
        "dnsPolicy/dnsConfig do Pod incorretas ou hostNetwork sem ClusterFirstWithHostNet.",
        "/etc/hosts dentro do container sobrepõe resolução."
      ],
      "immediate_actions": [
        "Verifique CoreDNS e logs: kubectl -n kube-system get pods -l k8s-app=kube-dns && kubectl -n kube-system logs deploy/coredns --tail=200",
        "Cheque Service/Endpoints: kubectl get svc,ep,endpointslices <service> -n <ns>",
        "Teste resolução no Pod: kubectl exec -it <pod> -n <ns> -- sh -c \"getent hosts <host> || nslookup <host> || dig +short <host>\"",
        "Valide NetworkPolicies liberando UDP/TCP 53 ao kube-dns.",
        "Teste FQDN completo do Service: <svc>.<ns>.svc.cluster.local."
      ],
      "best_practices": [
        "Use FQDN para Services internos e domínios absolutos para externos.",
        "Monitore latência/erros do CoreDNS; ajuste requests/limits e HPA.",
        "Mantenha ConfigMap do CoreDNS simples e validado.",
        "Evite entradas fixas em /etc/hosts nas imagens.",
        "Em hostNetwork, use dnsPolicy: ClusterFirstWithHostNet."
      ],
      "storage_diagnostics": [
        "Não se aplica a DNS; priorize logs do CoreDNS e verificação de NetworkPolicy."
      ],
      "corrections": [
        "Corrija typos e use FQDN .svc.cluster.local quando for Service.",
        "Ajuste selectors do Service para gerar Endpoints válidos.",
        "Permita egress UDP/TCP 53 (kube-dns) nas NetworkPolicies.",
        "Corrija forward/stubDomains no CoreDNS e faça rollout restart.",
        "Ajuste dnsPolicy/dnsConfig (ou ClusterFirstWithHostNet) conforme necessário.",
        "Remova entradas conflitantes em /etc/hosts na imagem/container."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod DNS policy misconfigured' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Service selector mismatch",
    "slug": "service-selector-mismatch",
    "aliases": [
      "Service",
      "Service selector mismatch",
      "mismatch",
      "no endpoints available",
      "selector"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Service selector mismatch' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "no endpoints available",
        "EndpointSlice has 0 endpoints"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Service selector mismatch' no Kubernetes?",
        "a": "'Service selector mismatch' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod readiness gate failed",
    "slug": "pod-readiness-gate-failed",
    "aliases": [
      "Pod",
      "Pod readiness gate failed",
      "ReadinessGates not satisfied",
      "failed",
      "gate",
      "readiness"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Pod readiness gate failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "ReadinessGates not satisfied",
        "condition .* not True"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Path/porta incorretos nas probes.",
        "Timeout/intervalo agressivo em apps lentas.",
        "TLS/autenticação exigidos pelo endpoint de saúde.",
        "Dependência externa lenta causando falha na saúde."
      ],
      "immediate_actions": [
        "Descreva o pod e veja falhas de probe.",
        "Teste manualmente o endpoint de saúde via kubectl exec (curl).",
        "Aumente temporariamente timeoutSeconds/periodSeconds/failureThreshold.",
        "Habilite startupProbe quando houver cold start."
      ],
      "best_practices": [
        "Separar liveness/readiness/startup com semânticas claras.",
        "Endpoints de saúde leves e independentes.",
        "Não acople readiness à saúde de dependências remotas sem retries.",
        "Logar motivos de unhealthy no app."
      ],
      "storage_diagnostics": [
        "Não aplicável; probes não usam storage."
      ],
      "corrections": [
        "Corrija paths/portas e autenticação.",
        "Ajuste janelas de timeout/period/failureThreshold.",
        "Implemente startupProbe para inicializações lentas.",
        "Desacople dependências ou trate timeouts/retries."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod readiness gate failed' no Kubernetes?",
        "a": "'Pod readiness gate failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CSI NodePublishVolume failed",
    "slug": "csi-nodepublishvolume-failed",
    "aliases": [
      "CSI",
      "CSI NodePublishVolume failed",
      "NodePublishVolume",
      "NodePublishVolume .* failed",
      "failed"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CSI NodePublishVolume failed' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "NodePublishVolume .* failed",
        "mount propagation not supported"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CSI NodePublishVolume failed' no Kubernetes?",
        "a": "'CSI NodePublishVolume failed' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ServiceAccount token projected volume error",
    "slug": "serviceaccount-token-projected-volume-error",
    "aliases": [
      "ServiceAccount",
      "ServiceAccount token projected volume error",
      "error",
      "projected",
      "token",
      "token expired",
      "volume"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'ServiceAccount token projected volume error' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "failed to fetch token",
        "token expired"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ServiceAccount token projected volume error' no Kubernetes?",
        "a": "'ServiceAccount token projected volume error' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Kubelet can't pull image (proxy)",
    "slug": "kubelet-can-t-pull-image-proxy",
    "aliases": [
      "Kubelet",
      "Kubelet can't pull image (proxy)",
      "Proxy Authentication Required",
      "can",
      "image",
      "proxy",
      "pull"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Kubelet can't pull image (proxy)' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "Proxy Authentication Required",
        "407 Proxy Authentication Required"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Kubelet can't pull image (proxy)' no Kubernetes?",
        "a": "'Kubelet can't pull image (proxy)' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node GPU device plugin error",
    "slug": "node-gpu-device-plugin-error",
    "aliases": [
      "GPU",
      "Node",
      "Node GPU device plugin error",
      "device",
      "error",
      "plugin"
    ],
    "category": "Node/Cluster",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Node GPU device plugin error' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NVIDIA device plugin failed",
        "failed to initialize NVML"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node GPU device plugin error' no Kubernetes?",
        "a": "'Node GPU device plugin error' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "AWS Permission Denied",
    "slug": "aws-permission-denied",
    "aliases": [
      "AWS",
      "AWS Permission Denied",
      "Denied",
      "Permission",
      "aws permission denied"
    ],
    "category": "AWS/EKS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'AWS Permission Denied' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "aws permission denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Monte com `readOnly: false` onde escreve e ajuste `fsGroup`/`runAsUser`.",
      "Corrija caminhos/`workingDir` e permissões de execução.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'AWS Permission Denied' no Kubernetes?",
        "a": "'AWS Permission Denied' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Unable to locate credentials. You can configure credentials by running \"aws configure\".",
    "slug": "unable-to-locate-credentials-you-can-configure-credentials-by-running-aws-configure",
    "aliases": [
      "Unable",
      "Unable to locate credentials. You can configure credentials by running \"aws configure\".",
      "You",
      "can",
      "configure",
      "credentials",
      "locate"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Unable to locate credentials. You can configure credentials by running \"aws configure\".' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "Unable to locate credentials. You can configure credentials by running \"aws configure\".",
        "Unable to locate credentials"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Unable to locate credentials. You can configure credentials by running \"aws configure\".' no Kubernetes?",
        "a": "'Unable to locate credentials. You can configure credentials by running \"aws configure\".' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Invalid Ingress API version",
    "slug": "invalid-ingress-api-version",
    "aliases": [
      "API",
      "Ingress",
      "Invalid",
      "Invalid Ingress API version",
      "version"
    ],
    "category": "Ingress",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Invalid Ingress API version' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "no matches for kind \"Ingress\" in version \"extensions/v1beta1\"",
        "no matches for kind \"Ingress\" in version \"networking.k8s.io/v1beta1\""
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Invalid Ingress API version' no Kubernetes?",
        "a": "'Invalid Ingress API version' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CRD kind not recognized",
    "slug": "crd-kind-not-recognized",
    "aliases": [
      "CRD",
      "CRD kind not recognized",
      "kind",
      "not",
      "recognized"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'CRD kind not recognized' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "re:no matches for kind \\\".*\\\" in version \\\".*\\\"",
        "error validating data: ValidationError: unknown field"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CRD kind not recognized' no Kubernetes?",
        "a": "'CRD kind not recognized' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "cert-manager: order failed to finalize",
    "slug": "cert-manager-order-failed-to-finalize",
    "aliases": [
      "cert-manager",
      "cert-manager: order failed to finalize",
      "failed",
      "finalize",
      "order"
    ],
    "category": "Certificates/ACME",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'cert-manager: order failed to finalize' geralmente indica um problema na categoria Certificates/ACME",
    "signals": {
      "log_keywords": [
        "cert-manager.* Failed to finalize order",
        "Order .* marked as invalid"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'cert-manager: order failed to finalize' no Kubernetes?",
        "a": "'cert-manager: order failed to finalize' geralmente indica um problema na categoria Certificates/ACME"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ACME rate limited",
    "slug": "acme-rate-limited",
    "aliases": [
      "429 urn:ietf:params:acme:error:rateLimited",
      "ACME",
      "ACME rate limited",
      "limited",
      "rate"
    ],
    "category": "Certificates/ACME",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ACME rate limited' geralmente indica um problema na categoria Certificates/ACME",
    "signals": {
      "log_keywords": [
        "429 urn:ietf:params:acme:error:rateLimited",
        "rateLimited: Error creating new order"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Certificado expirado ou CN/SAN não combinam com o host.",
        "Cadeia incompleta (intermediários ausentes).",
        "CA não confiável no cliente.",
        "Protocolos/ciphers incompatíveis."
      ],
      "immediate_actions": [
        "Inspecione o cert com openssl s_client a partir do pod.",
        "Verifique datas de validade e SANs.",
        "Inclua cadeia completa no servidor (fullchain).",
        "Instale/monte CA confiável no cliente quando necessário."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Reemitir/renovar certificado com SANs corretos.",
        "Instalar cadeia completa e CA confiável.",
        "Ajustar versões/ciphers suportados.",
        "Configurar truststore/keystore corretos no cliente."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ACME rate limited' no Kubernetes?",
        "a": "'ACME rate limited' geralmente indica um problema na categoria Certificates/ACME"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "HTTP-01 self-check failed",
    "slug": "http-01-self-check-failed",
    "aliases": [
      "HTTP-01",
      "HTTP-01 self-check failed",
      "failed",
      "self-check"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'HTTP-01 self-check failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Waiting for HTTP-01 challenge propagation: wrong status code '404'",
        "Challenge self check failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'HTTP-01 self-check failed' no Kubernetes?",
        "a": "'HTTP-01 self-check failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Service targetPort not found",
    "slug": "service-targetport-not-found",
    "aliases": [
      "Service",
      "Service targetPort not found",
      "found",
      "not",
      "re:spec\\.ports\\[\\d+\\]\\.targetPort: Not found",
      "targetPort"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Service targetPort not found' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "re:spec\\.ports\\[\\d+\\]\\.targetPort: Not found",
        "targetPort not found in Pod"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Service targetPort not found' no Kubernetes?",
        "a": "'Service targetPort not found' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "runAsNonRoot conflicts with image",
    "slug": "runasnonroot-conflicts-with-image",
    "aliases": [
      "conflicts",
      "image",
      "must runAsNonRoot",
      "runAsNonRoot",
      "runAsNonRoot conflicts with image",
      "with"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'runAsNonRoot conflicts with image' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "container has runAsNonRoot and image will run as root",
        "must runAsNonRoot"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'runAsNonRoot conflicts with image' no Kubernetes?",
        "a": "'runAsNonRoot conflicts with image' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Failed to fetch anonymous token (registry)",
    "slug": "failed-to-fetch-anonymous-token-registry",
    "aliases": [
      "Failed",
      "Failed to fetch anonymous token (registry)",
      "anonymous",
      "fetch",
      "registry",
      "token"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Failed to fetch anonymous token (registry)' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "failed to fetch anonymous token: unexpected status: 401 Unauthorized",
        "failed to authorize: rpc error: code = Unknown desc = failed to fetch anonymous token"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Failed to fetch anonymous token (registry)' no Kubernetes?",
        "a": "'Failed to fetch anonymous token (registry)' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Image pull TLS timeout",
    "slug": "image-pull-tls-timeout",
    "aliases": [
      "Image",
      "Image pull TLS timeout",
      "TLS",
      "pull",
      "timeout"
    ],
    "category": "Image/Registry",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Image pull TLS timeout' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "Client.Timeout exceeded while awaiting headers",
        "net/http: TLS handshake timeout"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Automatize renovação (ex.: cert-manager/ACME).",
        "Use SANs corretos e monitore expiração.",
        "Evite desativar verificação TLS; prefira arrumar a cadeia.",
        "Padronize políticas de TLS (mínimo TLS 1.2 quando possível)."
      ],
      "storage_diagnostics": [
        "Não aplicável; TLS é rede/segurança."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó",
      "kubectl get svc,endpoints,endpointslices -n <ns>",
      "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get svc,endpoints,endpointslices -n <ns>",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl port-forward pod/<pod> -n <ns> <localPort>:<containerPort>  # testar endpoint"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Corrija `selector`/labels do Service e confirme *endpoints*.",
      "Valide `targetPort` e que o app escuta na porta correta.",
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'Image pull TLS timeout' no Kubernetes?",
        "a": "'Image pull TLS timeout' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CNI bridge not found",
    "slug": "cni-bridge-not-found",
    "aliases": [
      "CNI",
      "CNI bridge not found",
      "bridge",
      "found",
      "not"
    ],
    "category": "CNI/Networking",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'CNI bridge not found' geralmente indica um problema na categoria CNI/Networking",
    "signals": {
      "log_keywords": [
        "cni0: link not found",
        "failed to setup network for sandbox: failed to find bridge cni0"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CNI bridge not found' no Kubernetes?",
        "a": "'CNI bridge not found' geralmente indica um problema na categoria CNI/Networking"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "iptables-restore failed",
    "slug": "iptables-restore-failed",
    "aliases": [
      "failed",
      "iptables-restore",
      "iptables-restore failed",
      "kube-proxy.* iptables-save failed"
    ],
    "category": "CNI/Networking",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'iptables-restore failed' geralmente indica um problema na categoria CNI/Networking",
    "signals": {
      "log_keywords": [
        "iptables-restore: line .* failed",
        "kube-proxy.* iptables-save failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'iptables-restore failed' no Kubernetes?",
        "a": "'iptables-restore failed' geralmente indica um problema na categoria CNI/Networking"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Calico IPAM allocation error",
    "slug": "calico-ipam-allocation-error",
    "aliases": [
      "Calico",
      "Calico IPAM allocation error",
      "IPAM",
      "allocation",
      "error"
    ],
    "category": "CNI/Networking",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Calico IPAM allocation error' geralmente indica um problema na categoria CNI/Networking",
    "signals": {
      "log_keywords": [
        "calico.* IPAM allocation error",
        "failed to assign IP address to pod"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Calico IPAM allocation error' no Kubernetes?",
        "a": "'Calico IPAM allocation error' geralmente indica um problema na categoria CNI/Networking"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Cilium not ready",
    "slug": "cilium-not-ready",
    "aliases": [
      "Cilium",
      "Cilium API unreachable",
      "Cilium not ready",
      "not",
      "ready"
    ],
    "category": "CNI/Networking",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Cilium not ready' geralmente indica um problema na categoria CNI/Networking",
    "signals": {
      "log_keywords": [
        "cilium.* is not ready",
        "Cilium API unreachable"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Cilium not ready' no Kubernetes?",
        "a": "'Cilium not ready' geralmente indica um problema na categoria CNI/Networking"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "AWS LB Controller AccessDenied",
    "slug": "aws-lb-controller-accessdenied",
    "aliases": [
      "AWS",
      "AWS LB Controller AccessDenied",
      "AccessDenied",
      "Controller"
    ],
    "category": "AWS/EKS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'AWS LB Controller AccessDenied' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "AccessDenied: User .* is not authorized to perform: elasticloadbalancing:CreateLoadBalancer",
        "re:failed to reconcile: .* AccessDenied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'AWS LB Controller AccessDenied' no Kubernetes?",
        "a": "'AWS LB Controller AccessDenied' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "AWS LB Controller: no suitable subnets",
    "slug": "aws-lb-controller-no-suitable-subnets",
    "aliases": [
      "AWS",
      "AWS LB Controller: no suitable subnets",
      "Controller",
      "subnets",
      "suitable"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'AWS LB Controller: no suitable subnets' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "failed to build load balancer due to no suitable subnets",
        "subnets not tagged for auto-discovery"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'AWS LB Controller: no suitable subnets' no Kubernetes?",
        "a": "'AWS LB Controller: no suitable subnets' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "AWS VPC CNI exhausted IPs",
    "slug": "aws-vpc-cni-exhausted-ips",
    "aliases": [
      "AWS",
      "AWS VPC CNI exhausted IPs",
      "CNI",
      "IPs",
      "VPC",
      "exhausted"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'AWS VPC CNI exhausted IPs' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "ipamd: prefix has no available addresses",
        "failed to assign an IP address to pod"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'AWS VPC CNI exhausted IPs' no Kubernetes?",
        "a": "'AWS VPC CNI exhausted IPs' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "IRSA invalid identity token",
    "slug": "irsa-invalid-identity-token",
    "aliases": [
      "IRSA",
      "IRSA invalid identity token",
      "InvalidIdentityToken",
      "identity",
      "invalid",
      "token"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'IRSA invalid identity token' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "InvalidIdentityToken",
        "could not assume role with web identity"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'IRSA invalid identity token' no Kubernetes?",
        "a": "'IRSA invalid identity token' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "IRSA OIDC provider not found",
    "slug": "irsa-oidc-provider-not-found",
    "aliases": [
      "IRSA",
      "IRSA OIDC provider not found",
      "OIDC",
      "found",
      "not",
      "provider"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'IRSA OIDC provider not found' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "No OpenIDConnect provider found in your account",
        "oidc: JWT signature verification failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'IRSA OIDC provider not found' no Kubernetes?",
        "a": "'IRSA OIDC provider not found' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ECR GetAuthorizationToken denied",
    "slug": "ecr-getauthorizationtoken-denied",
    "aliases": [
      "ECR",
      "ECR GetAuthorizationToken denied",
      "GetAuthorizationToken",
      "denied"
    ],
    "category": "AWS/EKS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'ECR GetAuthorizationToken denied' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "AccessDeniedException: User .* is not authorized to perform: ecr:GetAuthorizationToken",
        "cannot retrieve repository credentials"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'ECR GetAuthorizationToken denied' no Kubernetes?",
        "a": "'ECR GetAuthorizationToken denied' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Kubelet create shim task failed",
    "slug": "kubelet-create-shim-task-failed",
    "aliases": [
      "Kubelet",
      "Kubelet create shim task failed",
      "create",
      "failed",
      "shim",
      "task"
    ],
    "category": "Node/Cluster",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Kubelet create shim task failed' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "failed to create shim task",
        "OCI runtime create failed"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Kubelet create shim task failed' no Kubernetes?",
        "a": "'Kubelet create shim task failed' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Address already in use (container)",
    "slug": "address-already-in-use-container",
    "aliases": [
      "Address",
      "Address already in use (container)",
      "already",
      "container",
      "use"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Address already in use (container)' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "address already in use",
        "bind: address already in use"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Address already in use (container)' no Kubernetes?",
        "a": "'Address already in use (container)' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Init container failed",
    "slug": "init-container-failed",
    "aliases": [
      "Init",
      "Init container failed",
      "Init:CrashLoopBackOff",
      "Init:Error",
      "container",
      "failed"
    ],
    "category": "General",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Init container failed' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "Init:CrashLoopBackOff",
        "Init:Error"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Erro na inicialização (config/env faltando, dependência indisponível).",
        "Falha de permissões (arquivo/volume/porta).",
        "Probes agressivas derrubando o container.",
        "Comando/args/entrypoint incorretos."
      ],
      "immediate_actions": [
        "Ver logs atuais e anteriores: kubectl logs <pod> -n <ns> --all-containers --tail=300; kubectl logs <pod> -n <ns> --previous",
        "Descrever pod: kubectl describe pod <pod> -n <ns> e checar eventos.",
        "Ajustar temporariamente startupProbe/readinessProbe para isolar problema.",
        "Executar container interativo com comando seguro para investigar (sleep/sh)."
      ],
      "best_practices": [
        "Valide variáveis/Secrets/ConfigMaps antes do deploy.",
        "Use startupProbe para apps pesadas; dê tempo para aquecimento.",
        "Padronize healthchecks e exponha métricas para observabilidade.",
        "Mantenha logs estruturados para facilitar triagem."
      ],
      "storage_diagnostics": [
        "Se envolver volumes, verifique permissões, paths e montagem antes do processo iniciar."
      ],
      "corrections": [
        "Corrija configs/credenciais/URLs; reprovisione Secrets/ConfigMaps.",
        "Ajuste probes (periodSeconds, timeoutSeconds, failureThreshold).",
        "Conserte comando/args/entrypoint; valide em ambiente local.",
        "Garanta dependências acessíveis (DB/filas) e trate timeouts/retries."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Init container failed' no Kubernetes?",
        "a": "'Init container failed' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "StatefulSet PVC pending",
    "slug": "statefulset-pvc-pending",
    "aliases": [
      "PVC",
      "StatefulSet",
      "StatefulSet PVC pending",
      "pending"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'StatefulSet PVC pending' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "persistentvolumeclaim .* pending for StatefulSet",
        "pod has unbound immediate PersistentVolumeClaims"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Requests de CPU/memória altos e sem capacidade livre.",
        "Taints no nó sem tolerations correspondentes.",
        "Afinidade/antiafinidade/TopologySpread inviáveis.",
        "PVC pendente/StorageClass inexistente."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e leia a razão de Unschedulable.",
        "Compare requests com capacidade: kubectl get nodes -o wide e métricas.",
        "Revise tolerations/affinity/topologySpreadConstraints.",
        "Se houver PVC, confira: kubectl get pvc,pv -n <ns>."
      ],
      "best_practices": [
        "Requests proporcionais ao uso real; limite overcommit.",
        "Padronize tolerations e use NodeAffinity com rótulos coerentes.",
        "Use autoscaling de nós quando cabível.",
        "Valide StorageClass/PVC nos ambientes."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'StatefulSet PVC pending' no Kubernetes?",
        "a": "'StatefulSet PVC pending' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "HPA external metrics unavailable",
    "slug": "hpa-external-metrics-unavailable",
    "aliases": [
      "HPA",
      "HPA external metrics unavailable",
      "external",
      "metrics",
      "unavailable"
    ],
    "category": "Observability",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'HPA external metrics unavailable' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "unable to get external metric",
        "no metrics returned from external metrics API"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'HPA external metrics unavailable' no Kubernetes?",
        "a": "'HPA external metrics unavailable' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Prometheus rule parse error",
    "slug": "prometheus-rule-parse-error",
    "aliases": [
      "Prometheus",
      "Prometheus rule parse error",
      "error",
      "error loading rules",
      "parse",
      "rule"
    ],
    "category": "Observability",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Prometheus rule parse error' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "error loading rules",
        "parsing YAML file .* error"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Prometheus rule parse error' no Kubernetes?",
        "a": "'Prometheus rule parse error' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Alertmanager 403 or route error",
    "slug": "alertmanager-403-or-route-error",
    "aliases": [
      "403",
      "Alertmanager",
      "Alertmanager 403 or route error",
      "error",
      "route"
    ],
    "category": "Observability",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'Alertmanager 403 or route error' geralmente indica um problema na categoria Observability",
    "signals": {
      "log_keywords": [
        "unexpected status code 403 from Alertmanager",
        "no route found for path .* in Alertmanager"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Alertmanager 403 or route error' no Kubernetes?",
        "a": "'Alertmanager 403 or route error' geralmente indica um problema na categoria Observability"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Istio upstream reset 503",
    "slug": "istio-upstream-reset-503",
    "aliases": [
      "503",
      "503 UF,URX",
      "Istio",
      "Istio upstream reset 503",
      "reset",
      "upstream"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Istio upstream reset 503' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "503 UF,URX",
        "upstream connect error or disconnect/reset before headers"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Istio upstream reset 503' no Kubernetes?",
        "a": "'Istio upstream reset 503' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Istio xDS resource not found",
    "slug": "istio-xds-resource-not-found",
    "aliases": [
      "EDS: no endpoints",
      "Istio",
      "Istio xDS resource not found",
      "found",
      "not",
      "resource",
      "xDS"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Istio xDS resource not found' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "EDS: no endpoints",
        "RDS: route not found"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Istio xDS resource not found' no Kubernetes?",
        "a": "'Istio xDS resource not found' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "NGINX Ingress upstream timed out",
    "slug": "nginx-ingress-upstream-timed-out",
    "aliases": [
      "Ingress",
      "NGINX",
      "NGINX Ingress upstream timed out",
      "out",
      "timed",
      "upstream",
      "upstream timed out"
    ],
    "category": "Ingress",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'NGINX Ingress upstream timed out' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "upstream timed out",
        "110: Connection timed out while reading response header from upstream"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Ajuste `path`, `pathType` e backends; corrija o `Secret` TLS (tipo `kubernetes.io/tls`).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'NGINX Ingress upstream timed out' no Kubernetes?",
        "a": "'NGINX Ingress upstream timed out' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "NGINX header too large",
    "slug": "nginx-header-too-large",
    "aliases": [
      "NGINX",
      "NGINX header too large",
      "header",
      "large",
      "too"
    ],
    "category": "Ingress",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'NGINX header too large' geralmente indica um problema na categoria Ingress",
    "signals": {
      "log_keywords": [
        "upstream sent too big header",
        "header size too big"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'NGINX header too large' no Kubernetes?",
        "a": "'NGINX header too large' geralmente indica um problema na categoria Ingress"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "cert-manager DNS-01 propagation failed",
    "slug": "cert-manager-dns-01-propagation-failed",
    "aliases": [
      "DNS-01",
      "cert-manager",
      "cert-manager DNS-01 propagation failed",
      "failed",
      "propagation"
    ],
    "category": "Networking/DNS",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "Waiting for DNS-01 challenge propagation",
        "NXDOMAIN when querying TXT for _acme-challenge"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'cert-manager DNS-01 propagation failed' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Leader election lost",
    "slug": "leader-election-lost",
    "aliases": [
      "Leader",
      "Leader election lost",
      "election",
      "leaderelection lost",
      "lost"
    ],
    "category": "General",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Leader election lost' geralmente indica um problema na categoria General",
    "signals": {
      "log_keywords": [
        "leaderelection lost",
        "failed to renew lease"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Leader election lost' no Kubernetes?",
        "a": "'Leader election lost' geralmente indica um problema na categoria General"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "DNS not found / NXDOMAIN",
    "slug": "dns-not-found-nxdomain",
    "aliases": [
      "DNS",
      "DNS not found / NXDOMAIN",
      "NXDOMAIN",
      "found",
      "no such host",
      "not"
    ],
    "category": "Networking/DNS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo",
    "signals": {
      "log_keywords": [
        "no such host",
        "Name or service not known",
        "lookup .* on .*: no such host",
        "NXDOMAIN",
        "coredns .* plugin/forward: no upstream host"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FQDN/hostname incorreto ou sem sufixo de busca (.svc.cluster.local).",
        "Service inexistente ou sem endpoints (selectors não batem).",
        "CoreDNS com ConfigMap inválido (forward/stubDomains) ou pods em CrashLoop/sem recursos.",
        "NetworkPolicy bloqueando egress UDP/TCP 53 para kube-dns.",
        "Resolver upstream fora do ar/sem rota (firewall/VPC/peering/proxy).",
        "dnsPolicy/dnsConfig do Pod incorretas ou hostNetwork sem ClusterFirstWithHostNet.",
        "/etc/hosts dentro do container sobrepõe resolução."
      ],
      "immediate_actions": [
        "Verifique CoreDNS e logs: kubectl -n kube-system get pods -l k8s-app=kube-dns && kubectl -n kube-system logs deploy/coredns --tail=200",
        "Cheque Service/Endpoints: kubectl get svc,ep,endpointslices <service> -n <ns>",
        "Teste resolução no Pod: kubectl exec -it <pod> -n <ns> -- sh -c \"getent hosts <host> || nslookup <host> || dig +short <host>\"",
        "Valide NetworkPolicies liberando UDP/TCP 53 ao kube-dns.",
        "Teste FQDN completo do Service: <svc>.<ns>.svc.cluster.local."
      ],
      "best_practices": [
        "Use FQDN para Services internos e domínios absolutos para externos.",
        "Monitore latência/erros do CoreDNS; ajuste requests/limits e HPA.",
        "Mantenha ConfigMap do CoreDNS simples e validado.",
        "Evite entradas fixas em /etc/hosts nas imagens.",
        "Em hostNetwork, use dnsPolicy: ClusterFirstWithHostNet."
      ],
      "storage_diagnostics": [
        "Não se aplica a DNS; priorize logs do CoreDNS e verificação de NetworkPolicy."
      ],
      "corrections": [
        "Corrija typos e use FQDN .svc.cluster.local quando for Service.",
        "Ajuste selectors do Service para gerar Endpoints válidos.",
        "Permita egress UDP/TCP 53 (kube-dns) nas NetworkPolicies.",
        "Corrija forward/stubDomains no CoreDNS e faça rollout restart.",
        "Ajuste dnsPolicy/dnsConfig (ou ClusterFirstWithHostNet) conforme necessário.",
        "Remova entradas conflitantes em /etc/hosts na imagem/container."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl -n kube-system logs deploy/coredns --tail=200",
      "kubectl exec -it <pod> -n <ns> -- nslookup <host>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/coredns --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl exec -it <pod> -n <ns> -- nslookup <host>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Corrija o FQDN/hostname; confira `coredns` e DNS *upstream*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Corrija o FQDN/hostname e verifique se há registros A/CNAME/TXT válidos.",
      "Se for serviço K8s, confirme Service/Endpoints e o sufixo .svc.cluster.local.",
      "Cheque políticas de rede/egress e o acesso ao DNS upstream."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'DNS not found / NXDOMAIN' no Kubernetes?",
        "a": "Falhas de resolução DNS (ex.: 'no such host', 'NXDOMAIN') indicam que o nome não pode ser resolvido pelo CoreDNS ou pelo DNS externo"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "ImagePullBackOff / ErrImagePull",
    "slug": "imagepullbackoff-errimagepull",
    "aliases": [
      "Back-off pulling image",
      "ErrImagePull",
      "ImagePullBackOff",
      "ImagePullBackOff / ErrImagePull",
      "pull access denied"
    ],
    "category": "Image/Registry",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'ImagePullBackOff / ErrImagePull' geralmente indica um problema na categoria Image/Registry",
    "signals": {
      "log_keywords": [
        "Back-off pulling image",
        "ErrImagePull",
        "failed to pull and unpack image",
        "pull access denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Nome/tag inválidos, imagem inexistente, ou credenciais/egresso até o registry indisponíveis."
      ],
      "immediate_actions": [
        "Confirme image e tag/digest exatamente como publicadas.",
        "Se privado: crie Secret tipo docker-registry e referencie em imagePullSecrets.",
        "Teste no nó: crictl pull <image>:<tag> (ou docker/podman)."
      ],
      "best_practices": [
        "Use tags imutáveis (SHA digest) e política imagePullPolicy adequada.",
        "Automatize login/secret via controlador (Workload Identity/IRSA).",
        "Evite 'latest'; versionamento semântico ajuda rollback.",
        "Monitore taxa de erro de pull."
      ],
      "storage_diagnostics": [
        "Não aplicável diretamente; apenas confirme espaço em disco do nó para camadas da imagem."
      ],
      "corrections": [
        "Ajuste o nome totalmente qualificado (ex.: gcr.io/proj/app:1.2.3 / public.ecr.aws/...).",
        "Garanta permissões no registry e políticas de rede/egress liberadas.",
        "Use imagePullPolicy: IfNotPresent (ou Always onde fizer sentido)."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl describe pod <pod> -n <ns> | grep -i image",
      "crictl pull <image>:<tag>  # executar no nó"
    ],
    "deep_dive": {
      "k8s_commands": [
        "crictl pull <image>:<tag>  # executar no nó",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pod <pod> -n <ns> | grep -i image",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "Falha ao puxar imagem",
          "expr": "sum by (namespace,pod) (kube_pod_container_status_waiting_reason{reason=~\"ImagePullBackOff|ErrImagePull\"})"
        }
      ]
    },
    "remediations": [
      "Corrija `command`/`args`/`entrypoint`; torne scripts executáveis.",
      "Revise *probes* e aumente tempos/thresholds conforme necessidade.",
      "Corrija nome/tag da imagem e *imagePullSecrets*.",
      "Valide conectividade de rede ao *registry* (egresso/HTTP proxy).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Garanta imagePullSecrets corretos e acesso ao registry.",
      "Valide tag/digest e disponibilidade da imagem no registry."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "A imagem é privada? Há imagePullSecrets configurado?",
      "A tag/digest existe no registry e há conectividade/egresso?"
    ],
    "faq": [
      {
        "q": "O que significa 'ImagePullBackOff / ErrImagePull' no Kubernetes?",
        "a": "'ImagePullBackOff / ErrImagePull' geralmente indica um problema na categoria Image/Registry"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "OOMKilled (Out Of Memory)",
    "slug": "oomkilled-out-of-memory",
    "aliases": [
      "Memory",
      "OOMKilled",
      "OOMKilled (Out Of Memory)",
      "Out",
      "Signal: 9 (OOMKilled)"
    ],
    "category": "Pod Runtime",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'OOMKilled (Out Of Memory)' geralmente indica um problema na categoria Pod Runtime",
    "signals": {
      "log_keywords": [
        "OOMKilled",
        "Signal: 9 (OOMKilled)"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Limite de memória (resources.limits.memory) menor que o uso real do processo.",
        "Spikes de alocação (cache, buffers, JIT ou GC) não refletidos em requests.",
        "Vazamentos ou memory bloat (por exemplo: grandes batches, buffers não liberados)."
      ],
      "immediate_actions": [
        "Verifique o motivo exato do OOM nos eventos: kubectl describe pod <pod> -n <ns> (campo Last State / Reason).",
        "Confira reinícios: kubectl get pod <pod> -n <ns> -o wide e kubectl logs <pod> -n <ns> --previous.",
        "Observe consumo: instale metrics-server e rode kubectl top pod <pod> -n <ns>.",
        "Se JVM: adicione flags que respeitem cgroups (ex.: -XX:+UseContainerSupport -XX:MaxRAMPercentage=75);",
        "se Node.js: ajuste --max-old-space-size; se Python: reduza workers/batch size."
      ],
      "best_practices": [
        "Defina requests/limits realistas com base em métricas históricas (P95/P99).",
        "Ative HPA/VPA quando aplicável; use limites para evitar ruído entre pods.",
        "Instrumente a aplicação (heap profiles) e monitore GC/leaks.",
        "Use liveness/readiness/startup probes para estabilidade durante picos."
      ],
      "storage_diagnostics": [
        "Não se aplica diretamente; verifique apenas se volumes temporários não induzem swap em memória."
      ],
      "corrections": [
        "Dimensione corretamente: aumente requests para o pico de uso e limits com margem (10–30%).",
        "Otimize a aplicação: reduza paralelismo, batch size, buffers; corrija vazamentos.",
        "Autoscaling: considere Vertical Pod Autoscaler (VPA) e HPA baseado em memória onde aplicável.",
        "Probes: evite que liveness mate o pod durante warmup — use startupProbe e tempos mais generosos."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl top pod <pod> -n <ns>",
      "kubectl top node"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous",
        "kubectl top node",
        "kubectl top pod <pod> -n <ns>"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "OOMKilled por pod (últimas 6h)",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_last_terminated_reason{reason=\"OOMKilled\"}[6h]))"
        },
        {
          "title": "Uso de memória por container",
          "expr": "avg_over_time(container_memory_working_set_bytes[5m])"
        }
      ]
    },
    "remediations": [
      "Ajuste `resources.requests/limits.memory` baseado no pico observado.",
      "Reduza *batch size*, paralelismo e *caches*; aplique flags de memória do runtime (JVM/Node/Python).",
      "Implemente `startupProbe` para proteger a inicialização e evite *liveness* prematura.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Ajuste requests/limits de CPU/Memória, comandos/args e dependências iniciais.",
      "Cheque volumes/paths/permissions exigidos pelo container."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Há picos de CPU/Memória ou erros específicos nos logs?",
      "As probes estão ajustadas (startup/liveness/readiness)?"
    ],
    "faq": [
      {
        "q": "O que significa 'OOMKilled (Out Of Memory)' no Kubernetes?",
        "a": "'OOMKilled (Out Of Memory)' geralmente indica um problema na categoria Pod Runtime"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Pod unschedulable (taints/resources)",
    "slug": "pod-unschedulable-taints-resources",
    "aliases": [
      "Pod",
      "Pod unschedulable (taints/resources)",
      "insufficient cpu",
      "insufficient memory",
      "node(s) had taint",
      "taints/resources",
      "unschedulable"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Pod unschedulable (taints/resources)' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "0/\\d+ nodes are available",
        "insufficient cpu",
        "insufficient memory",
        "node(s) had taint"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Falta de recursos no cluster, requests superestimados, taints sem tolerations, affinity/selector incompatíveis."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e leia a razão de Unschedulable.",
        "Compare requests com capacidade: kubectl get nodes -o wide e métricas.",
        "Revise tolerations/affinity/topologySpreadConstraints.",
        "Se houver PVC, confira: kubectl get pvc,pv -n <ns>."
      ],
      "best_practices": [
        "Requests proporcionais ao uso real; limite overcommit.",
        "Padronize tolerations e use NodeAffinity com rótulos coerentes.",
        "Use autoscaling de nós quando cabível.",
        "Valide StorageClass/PVC nos ambientes."
      ],
      "storage_diagnostics": [
        "Para PVC: confirme StorageClass, quota e disponibilidade de PVs."
      ],
      "corrections": [
        "Reduza requests/limits para right-sizing ou escale o cluster.",
        "Ajuste nodeSelector/nodeAffinity para rótulos realmente presentes; adicione tolerations quando houver taints.",
        "Revise topologySpreadConstraints e PodDisruptionBudget que podem bloquear o agendamento."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Reduza *requests* ou escale nós; habilite/ajuste *Cluster Autoscaler*.",
      "Ajuste `nodeSelector`/`affinity`/`tolerations` para casar com os nós disponíveis.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Pod unschedulable (taints/resources)' no Kubernetes?",
        "a": "'Pod unschedulable (taints/resources)' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "RBAC: Forbidden / Unauthorized",
    "slug": "rbac-forbidden-unauthorized",
    "aliases": [
      "Forbidden",
      "RBAC",
      "RBAC: Forbidden / Unauthorized",
      "Unauthorized"
    ],
    "category": "RBAC/Auth",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'RBAC: Forbidden / Unauthorized' geralmente indica um problema na categoria RBAC/Auth",
    "signals": {
      "log_keywords": [
        "Error from server (Forbidden)",
        "User .* cannot .* resource .* in API group .*",
        "Unauthorized"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
      "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl auth can-i <verb> <resource> --as=system:serviceaccount:<ns>:<sa>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get role,rolebinding,clusterrole,clusterrolebinding -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Crie/ajuste `Role`/`ClusterRole` e *bindings*; valide com `kubectl auth can-i`.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'RBAC: Forbidden / Unauthorized' no Kubernetes?",
        "a": "'RBAC: Forbidden / Unauthorized' geralmente indica um problema na categoria RBAC/Auth"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "PVC pending / volume not bound",
    "slug": "pvc-pending-volume-not-bound",
    "aliases": [
      "PVC",
      "PVC pending / volume not bound",
      "bound",
      "not",
      "pending",
      "persistentvolumeclaim .* pending",
      "volume"
    ],
    "category": "Storage/CSI",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'PVC pending / volume not bound' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "pod has unbound immediate PersistentVolumeClaims",
        "persistentvolumeclaim .* pending"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Requests de CPU/memória altos e sem capacidade livre.",
        "Taints no nó sem tolerations correspondentes.",
        "Afinidade/antiafinidade/TopologySpread inviáveis.",
        "PVC pendente/StorageClass inexistente."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e leia a razão de Unschedulable.",
        "Compare requests com capacidade: kubectl get nodes -o wide e métricas.",
        "Revise tolerations/affinity/topologySpreadConstraints.",
        "Se houver PVC, confira: kubectl get pvc,pv -n <ns>."
      ],
      "best_practices": [
        "Requests proporcionais ao uso real; limite overcommit.",
        "Padronize tolerations e use NodeAffinity com rótulos coerentes.",
        "Use autoscaling de nós quando cabível.",
        "Valide StorageClass/PVC nos ambientes."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        },
        {
          "title": "PVCs não bound",
          "expr": "sum by (namespace,persistentvolumeclaim) (kube_persistentvolumeclaim_status_phase{phase!~\"Bound|Lost\"})"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'PVC pending / volume not bound' no Kubernetes?",
        "a": "'PVC pending / volume not bound' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "CSI attach/mount failed (EBS/EFS)",
    "slug": "csi-attach-mount-failed-ebs-efs",
    "aliases": [
      "AttachVolume.Attach failed",
      "CSI",
      "CSI attach/mount failed (EBS/EFS)",
      "EBS/EFS",
      "MountVolume.Mount failed",
      "attach/mount",
      "failed"
    ],
    "category": "Storage/CSI",
    "severity": "high",
    "risk_score": 0.8,
    "overview": "'CSI attach/mount failed (EBS/EFS)' geralmente indica um problema na categoria Storage/CSI",
    "signals": {
      "log_keywords": [
        "AttachVolume.Attach failed",
        "MountVolume.Mount failed",
        "rpc error: code = Internal desc"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Credenciais/secret do CSI inválidos.",
        "Permissões de filesystem (UID/GID) incompatíveis.",
        "AccessMode incompatível (ex.: ReadWriteOnce em múltiplos pods).",
        "Path de montagem incorreto ou readOnly inesperado."
      ],
      "immediate_actions": [
        "Descreva o pod e o PVC/PV para eventos do CSI.",
        "Confirme fsGroup/runAsUser e permissões do path.",
        "Valide secrets/params do volume (ex.: NFS, S3, Filestore).",
        "Teste montagem com pod utilitário (busybox/alpine)."
      ],
      "best_practices": [
        "Padronize fsGroup e permissões.",
        "Documente AccessModes por tipo de workload.",
        "Evite execução como root quando possível; use SecurityContext.",
        "Monitore erros do driver CSI."
      ],
      "storage_diagnostics": [
        "Verifique PVC/PV/StorageClass: kubectl get pvc,pv,sc -A e kubectl describe pvc <name> -n <ns>.",
        "Logs do driver CSI (no kube-system): kubectl logs deploy/csi- -n kube-system --tail=200."
      ],
      "corrections": [
        "Combine accessModes/storageClassName/zona com o nó onde o pod será executado (node affinity do volume).",
        "Para Multi-Attach, use RWX (por ex., EFS/NFS) ou ReadWriteOncePod se suportado.",
        "Em MountVolume falhas: confira secrets/configmaps existentes e subPath corretos; verifique permissões."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get pvc,pv,sc -A",
      "kubectl describe pvc <pvc> -n <ns>",
      "kubectl -n kube-system logs deploy/csi-* --tail=200"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl -n kube-system logs deploy/csi-* --tail=200",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl describe pvc <pvc> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl get pvc,pv,sc -A",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Garanta `PVC` em estado `Bound` e `StorageClass` correta.",
      "Para *Multi-Attach*: use RWX (EFS/NFS) ou mude a estratégia para `ReadWriteOncePod`.",
      "Sincronize *zona/zone* do volume com o nó (*volume node affinity*).",
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Valide StorageClass, provisioner e parâmetros (zona, fsType, iops).",
      "Confirme acesso do Node ao backend (EBS/EFS/NFS) e permissões."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "O PVC está em estado Bound? Qual StorageClass e zona?",
      "Há indícios de multi-attach ou permissões no backend (EBS/EFS/NFS)?"
    ],
    "faq": [
      {
        "q": "O que significa 'CSI attach/mount failed (EBS/EFS)' no Kubernetes?",
        "a": "'CSI attach/mount failed (EBS/EFS)' geralmente indica um problema na categoria Storage/CSI"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Node NotReady / kubelet issues",
    "slug": "node-notready-kubelet-issues",
    "aliases": [
      "KubeletNotReady",
      "Node",
      "Node NotReady / kubelet issues",
      "NodeNotReady",
      "NotReady",
      "issues",
      "kubelet"
    ],
    "category": "Node/Cluster",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Node NotReady / kubelet issues' geralmente indica um problema na categoria Node/Cluster",
    "signals": {
      "log_keywords": [
        "NodeNotReady",
        "KubeletNotReady",
        "PLEG is not healthy"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous",
      "kubectl get nodes -o wide",
      "kubectl describe node <node>"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe node <node>",
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get nodes -o wide",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Libere recursos (disco/memória), reponha nós ruins e ajuste *eviction thresholds*.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?",
      "Existem taints/tolerations, affinities ou selectors restringindo scheduling?",
      "O cluster tem recursos suficientes agora? Há quotas ou PDBs bloqueando?"
    ],
    "faq": [
      {
        "q": "O que significa 'Node NotReady / kubelet issues' no Kubernetes?",
        "a": "'Node NotReady / kubelet issues' geralmente indica um problema na categoria Node/Cluster"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "STS AssumeRole/IRSA failure",
    "slug": "sts-assumerole-irsa-failure",
    "aliases": [
      "AssumeRole/IRSA",
      "InvalidIdentityToken",
      "STS",
      "STS AssumeRole/IRSA failure",
      "failure"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'STS AssumeRole/IRSA failure' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "InvalidIdentityToken",
        "AccessDenied: Not authorized to perform sts:AssumeRole",
        "could not assume role with web identity"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "Configuração incorreta de recursos, rede ou storage.",
        "Dependência externa indisponível (DB/filas/serviços).",
        "Permissões insuficientes (RBAC/FS/Secrets).",
        "Probes/healthchecks agressivos derrubando o container."
      ],
      "immediate_actions": [
        "kubectl describe pod <pod> -n <ns> e analise eventos.",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300 e --previous.",
        "Valide Service/Endpoints/NetworkPolicies.",
        "Cheque requests/limits e uso real com kubectl top.",
        "Revise Secrets/ConfigMaps/variáveis de ambiente."
      ],
      "best_practices": [
        "Versionamento e rollback fáceis; evite 'latest'.",
        "Observabilidade: métricas, logs estruturados e tracing.",
        "Requests/limits baseados em dados; autoscaling quando cabível.",
        "RBAC mínimo necessário e segurança por padrão."
      ],
      "storage_diagnostics": [
        "Se houver volumes, valide PVC/PV/driver CSI, accessModes e permissões."
      ],
      "corrections": [
        "Ajustar config/recursos conforme achados de eventos/logs.",
        "Consertar endpoints/NetworkPolicies e dependências externas.",
        "Corrigir permissões (RBAC/fsGroup) e secrets ausentes.",
        "Ajustar probes/timeouts e parâmetros de inicialização."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'STS AssumeRole/IRSA failure' no Kubernetes?",
        "a": "'STS AssumeRole/IRSA failure' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "AWS KMS permission / S3 403",
    "slug": "aws-kms-permission-s3-403",
    "aliases": [
      "403",
      "403 Forbidden",
      "AWS",
      "AWS KMS permission / S3 403",
      "AccessDenied: Access Denied",
      "AccessDeniedException: KMS",
      "KMS",
      "permission"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'AWS KMS permission / S3 403' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "AccessDeniedException: KMS",
        "403 Forbidden",
        "AccessDenied: Access Denied"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FS sem permissões para o usuário do container (EACCES/EPERM).",
        "SecurityContext faltando (runAsUser/fsGroup).",
        "Montagem readOnly onde é necessário escrita.",
        "RBAC insuficiente ao acessar API do Kubernetes."
      ],
      "immediate_actions": [
        "Verifique eventos e logs de erro EACCES/EPERM.",
        "Inspecione securityContext do pod e permissões do path montado.",
        "Confirme se o ServiceAccount possui Roles/ClusterRoles adequados.",
        "Teste escrita/leitura com kubectl exec."
      ],
      "best_practices": [
        "Definir UID/GID explícitos e fsGroup para volumes.",
        "Princípio do menor privilégio no RBAC.",
        "Evitar rodar como root; use capabilities mínimas.",
        "Documentar paths de escrita exigidos pela app."
      ],
      "storage_diagnostics": [
        "Checar perms/ownership no PV/NFS/Filestore e mapeamento UID/GID."
      ],
      "corrections": [
        "Ajustar fsGroup/runAsUser e permissões do filesystem.",
        "Alterar montagem para read-write quando necessário.",
        "Conceder RBAC mínimo para operações requeridas.",
        "Rebuild da imagem se necessário para corrigir usuário/grupo."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'AWS KMS permission / S3 403' no Kubernetes?",
        "a": "'AWS KMS permission / S3 403' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  },
  {
    "name": "Route53 record not found / propagation",
    "slug": "route53-record-not-found-propagation",
    "aliases": [
      "NXDOMAIN when querying",
      "Route53",
      "Route53 record not found / propagation",
      "found",
      "not",
      "propagation",
      "record"
    ],
    "category": "AWS/EKS",
    "severity": "medium",
    "risk_score": 0.5,
    "overview": "'Route53 record not found / propagation' geralmente indica um problema na categoria AWS/EKS",
    "signals": {
      "log_keywords": [
        "NXDOMAIN when querying",
        "record not found in hosted zone"
      ],
      "symptoms": []
    },
    "guidance": {
      "common_causes": [
        "FQDN/hostname incorreto ou sem sufixo de busca (.svc.cluster.local).",
        "Service inexistente ou sem endpoints (selectors não batem).",
        "CoreDNS com ConfigMap inválido (forward/stubDomains) ou pods em CrashLoop/sem recursos.",
        "NetworkPolicy bloqueando egress UDP/TCP 53 para kube-dns.",
        "Resolver upstream fora do ar/sem rota (firewall/VPC/peering/proxy).",
        "dnsPolicy/dnsConfig do Pod incorretas ou hostNetwork sem ClusterFirstWithHostNet.",
        "/etc/hosts dentro do container sobrepõe resolução."
      ],
      "immediate_actions": [
        "Verifique CoreDNS e logs: kubectl -n kube-system get pods -l k8s-app=kube-dns && kubectl -n kube-system logs deploy/coredns --tail=200",
        "Cheque Service/Endpoints: kubectl get svc,ep,endpointslices <service> -n <ns>",
        "Teste resolução no Pod: kubectl exec -it <pod> -n <ns> -- sh -c \"getent hosts <host> || nslookup <host> || dig +short <host>\"",
        "Valide NetworkPolicies liberando UDP/TCP 53 ao kube-dns.",
        "Teste FQDN completo do Service: <svc>.<ns>.svc.cluster.local."
      ],
      "best_practices": [
        "Use FQDN para Services internos e domínios absolutos para externos.",
        "Monitore latência/erros do CoreDNS; ajuste requests/limits e HPA.",
        "Mantenha ConfigMap do CoreDNS simples e validado.",
        "Evite entradas fixas em /etc/hosts nas imagens.",
        "Em hostNetwork, use dnsPolicy: ClusterFirstWithHostNet."
      ],
      "storage_diagnostics": [
        "Não se aplica a DNS; priorize logs do CoreDNS e verificação de NetworkPolicy."
      ],
      "corrections": [
        "Corrija typos e use FQDN .svc.cluster.local quando for Service.",
        "Ajuste selectors do Service para gerar Endpoints válidos.",
        "Permita egress UDP/TCP 53 (kube-dns) nas NetworkPolicies.",
        "Corrija forward/stubDomains no CoreDNS e faça rollout restart.",
        "Ajuste dnsPolicy/dnsConfig (ou ClusterFirstWithHostNet) conforme necessário.",
        "Remova entradas conflitantes em /etc/hosts na imagem/container."
      ],
      "validation": [
        "Após aplicar correções, reimplante e verifique: kubectl get pods,events -n <ns> até o pod ficar Ready."
      ]
    },
    "quick_checks": [
      "kubectl get events -A --sort-by=.lastTimestamp",
      "kubectl describe pod <pod> -n <ns>",
      "kubectl logs <pod> -n <ns> --all-containers --tail=300",
      "kubectl logs <pod> -n <ns> --previous"
    ],
    "deep_dive": {
      "k8s_commands": [
        "kubectl describe pod <pod> -n <ns>",
        "kubectl get deploy,sts,ds -n <ns>",
        "kubectl get events -A --sort-by=.lastTimestamp",
        "kubectl get pods -n <ns> -o wide",
        "kubectl logs <pod> -n <ns> --all-containers --tail=200",
        "kubectl logs <pod> -n <ns> --all-containers --tail=300",
        "kubectl logs <pod> -n <ns> --previous"
      ],
      "observability_queries": [
        {
          "title": "Pods aguardando / motivo (kube-state-metrics)",
          "expr": "sum by (namespace,pod,reason) (kube_pod_container_status_waiting_reason)"
        },
        {
          "title": "Reinícios por pod",
          "expr": "sum by (namespace,pod) (increase(kube_pod_container_status_restarts_total[1h]))"
        }
      ]
    },
    "remediations": [
      "Revise *trust policy* do IAM Role (IRSA), políticas do driver CSI e permissões S3/KMS.",
      "Reaplique o manifesto e confirme que o *pod* fica `Ready` sem reinícios.",
      "Identifique a causa nos eventos/logs e aplique o ajuste sugerido.",
      "Reimplante o recurso se necessário e confirme que o status mudou para Ready.",
      "Habilite OIDC do cluster (IRSA) e vincule a policy correta à ServiceAccount.",
      "Ajuste permissões IAM com base na ação negada (CloudTrail)."
    ],
    "questions_to_ask": [
      "O problema ocorre em todos os pods ou apenas em alguns?",
      "Quando começou e após qual mudança (deploy, config, autoscaling, dependências)?",
      "Qual o namespace, pod e container afetados?",
      "Há eventos recentes relevantes (describe/events)?"
    ],
    "faq": [
      {
        "q": "O que significa 'Route53 record not found / propagation' no Kubernetes?",
        "a": "'Route53 record not found / propagation' geralmente indica um problema na categoria AWS/EKS"
      },
      {
        "q": "Quais sinais devo observar nos logs e eventos?",
        "a": "Use os padrões de log/keywords e rode os comandos de diagnóstico listados em 'quick_checks'."
      },
      {
        "q": "Como validar que a correção funcionou?",
        "a": "Veja os passos em 'guidance.validation' e confirme o pod em estado Ready sem reinícios."
      }
    ],
    "answer_template": "Se a sua dúvida for sobre {name}, aqui vai um roteiro objetivo:\n• Sinais/indicadores: {signals}\n• Perguntas rápidas: {ask_next}\n• Checks imediatos: {quick_checks}\n• Caminho de correção: {remediations}\n• Como validar: {validation}\nObservação: use como guia mesmo quando o erro ainda não aparece nos pods."
  }
]